–°–ø–æ–π–ª–µ—Ä –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ —á–∏—Ç–∞—é—â–∏—Ö - —è –ø—Ä–æ—Å—Ç–æ —Å–∫–∞—á–∞—é DeepSeek –∏ –∑–∞–ø—É—â—É –µ–≥–æ —á–µ—Ä–µ–∑ llama.cpp, –Ω–∞ –∫–∞–∫—É—é-–ª–∏–±–æ –Ω–∞—É—á–Ω—É—é –Ω–æ–≤–∏–∑–Ω—É —ç—Ç–æ—Ç –ø–æ—Å—Ç —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –Ω–µ –ø—Ä–µ—Ç–µ–Ω–¥—É–µ—Ç. –ó–∞—á–µ–º —ç—Ç–æ –Ω–∞ —Ö–∞–±—Ä–µ? –ü—Ä–æ—Å—Ç–æ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö –∫ –ø–æ—Å—Ç—É "–ú–∞–Ω—É–∞–ª –ø–æ –∑–∞–ø—É—Å–∫—É –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏ DeepSeek-R1 –ª–æ–∫–∞–ª—å–Ω–æ" —è —Å–ø—Ä–æ—Å–∏–ª "–∏–º–µ–µ—Ç–µ –ª–∏ —Å–º—ã—Å–ª –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–∞ —Ç–æ–º –∂–µ–ª–µ–∑–µ, —á—Ç–æ —É –º–µ–Ω—è –µ—Å—Ç—å", –∏ –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ "–ø–ª—é—Å–∏–∫–æ–≤" –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –∫–æ–º—É-–Ω–∏–±—É–¥—å —ç—Ç–æ –±—É–¥–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ, –Ω—É –∞ —Ä–∞–∑ –ø–∞–∑–∑–ª —Å–ª–æ–∂–∏–ª—Å—è ‚Äî –æ —Ç–æ–º –∏ –ø–æ—Å—Ç, –ø—Ä–æ—Å—Ç–æ —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç–æ–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É. –ò –¥–∞, —è –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –¥–∏–ª–µ—Ç–∞–Ω—Ç –≤ LLM, —ç—Ç–æ –ø–µ—Ä–≤–∞—è "–ø—Ä–∏—Å—Ç—Ä–µ–ª–∫–∞", —Ç–∞–∫ —á—Ç–æ "—Ç—É—Ç–æ—Ä–∏–∞–ª–æ–º" –æ—Ñ–æ—Ä–º–ª—è—Ç—å –ø–æ—Å—Ç –Ω–µ –±—É–¥—É.

–í—Ç–æ—Ä–æ–π —Å–ø–æ–π–ª–µ—Ä - –¥–∞ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç. –ù–æ –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ. –ù–æ —Ä–∞–±–æ—Ç–∞–µ—Ç.

## –ñ–µ–ª–µ–∑–æ

–£–ø—Ä–∞–∂–Ω—è—Ç—å—Å—è —è –±—É–¥—É –≤–æ—Ç –Ω–∞ —Ç–∞–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:
HP z8 G4 —Ä–∞–±–æ—á–∞—è —Å—Ç–∞–Ω—Ü–∏—è –ø—Ä–∏–º–µ—Ä–Ω–æ –ø—è—Ç–∏–ª–µ—Ç–Ω–µ–π –¥–∞–≤–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Å—Ç–æ–∫–æ–≤–æ–º –≤–∞—Ä–∏–∞–Ω—Ç–µ.
–î–≤–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ [Xeon Gold 6132](https://www.intel.de/content/www/de/de/products/sku/123541/intel-xeon-gold-6132-processor-19-25m-cache-2-60-ghz/specifications.html), 768 GB –ø–∞–º—è—Ç–∏ DDR4, —Ç–µ—Ä–∞–±–∞–π—Ç–Ω—ã–π NVMe SSD Samsung. 
. 
CPU-z –≤—ã–¥–∞—ë—Ç –º–Ω–µ 10000+ –ø–æ–ø—É–≥–∞–µ–≤:
![](https://habrastorage.org/webt/d_/yh/_p/d_yh_p0en_whozfuj99jqozqxsy.png)
–ü–∞–º—è—Ç—å –Ω–∞–±—Ä–∞–Ω–∞ –ø–ª–∞—à–∫–∞–º–∏ –ø–æ 64 –ì–ë, –∏—Ö —Ç–∞–º –¥–≤–µ–Ω–∞–¥—Ü–∞—Ç—å —à—Ç—É–∫:
![](https://habrastorage.org/webt/on/5g/v0/on5gv0rl5mzjikwejwxcuempii0.png)
–ë–µ–Ω—á–º–∞—Ä–∫ –ø–∞–º—è—Ç–∏ –∏ –∫–µ—à–∞ —è —É–∂–µ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ö –ø–æ–∫–∞–∑—ã–≤–∞–ª, –≤—ã–Ω–µ—Å—É —Å—é–¥–∞ —Ç–æ–∂–µ:
![](https://habrastorage.org/webt/r2/pr/ru/r2prrupxqhuzmzpu60l2rporwd0.png)
–í–∏–¥–µ–æ–∫–∞—Ä—Ç —Ç–∞–º –¥–≤–µ - NVidia Quadro K4200 –¥–∞ AMD Radeon Pro WX 9100, –Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∫–∞–∫ GPU –æ–Ω–∏ –Ω–µ –±—É–¥—É—Ç (—Å–º—ã—Å–ª–∞ –≤ –æ–±—â–µ–º –Ω–µ—Ç).
![](https://habrastorage.org/webt/pn/yp/nx/pnypnxaxpnehsvrmwgw8nxb-wau.png)
–î–∏—Å–∫:
![](https://habrastorage.org/webt/gu/xu/sy/guxusypp7qprdup0zmk7_jycr7o.png)
–í–æ–æ–±—â–µ –∏—Ö —Ç–∞–º —á–µ—Ç—ã—Ä–µ —Ç–∞–∫–∏—Ö, –Ω–æ –≤ —Ä–µ–π–¥ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –Ω–µ –±—É–¥—É, –º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑ –≥—Ä—É–∑–∏—Ç—Å—è –≤ –ø–∞–º—è—Ç—å, –≤ –ø—Ä–∏–Ω—Ü–∏–ø–µ –∏ —Ç–∞–∫ –Ω–æ—Ä–º.

–ö–æ–º–ø –æ—Å—Ç–∞–ª—Å—è –∫–∞–∫ —Ç–µ—Å—Ç–æ–≤—ã–π –æ—Ç –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞,  –≤ –∫–æ—Ç–æ—Ä–æ–º –Ω–∞–¥–æ –±—ã–ª–æ –±—ã—Å—Ç—Ä–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–æ–ª—Ç–µ—Ä–∞–±–∞–π—Ç–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫, –ø—Ä–∏–ª–µ—Ç–∞—é—â–∏—Ö –æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–∫–æ—Ä–æ—Å—Ç–Ω—ã—Ö –∫–∞–º–µ—Ä, —Ç–µ–ø–µ—Ä—å –ø—Ä–æ—Å—Ç–æ –ø—ã–ª–∏—Ç—Å—è –ø–æ–¥ —Å—Ç–æ–ª–æ–º –∏ —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π.

## –°–∫–∞—á–∏–≤–∞–µ–º DeepSeek 

–Ø –±—ã –º–æ–≥ –ø—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—Å–∞—Ç—å "—Å–∫–∞—á–∞–π—Ç–µ DeepSeek R1 Q8_0", –Ω–æ —Ç—É—Ç –µ—Å—Ç—å –Ω–µ–±–æ–ª—å—à–æ–π –Ω—é–∞–Ω—Å. –î–µ–ª–æ –≤ —Ç–æ–º, —á—Ç–æ –Ω–∞ —Ä–∞–±–æ—Ç–µ —É –º–µ–Ω—è –≤—Å–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—ã —Å—Ç–∞—Ä–∞—Ç–µ–ª—å–Ω–æ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã –∑–ª—ã–º –∞–¥–º–∏–Ω–æ–º, –æ—Ö—Ä–∞–Ω—è—é—â–∏–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å. –£ –º–µ–Ω—è –Ω–µ—Ç –æ–Ω–ª–∞–π–Ω –¥–æ—Å—Ç—É–ø–∞ –Ω–∏ –∫ ChatGPT, –Ω–∏ –∫ Perplexity, –Ω–∏ –∫ DeepSeek, —Ä–∞–≤–Ω–æ –∫–∞–∫ –≤—Å–µ –¥—Ä–æ–ø–±–æ–∫—Å—ã, –≥—É–≥–ª–æ–¥—Ä–∞–π–≤—ã, –æ–Ω–ª–∞–π–Ω –∑–∞–º–µ—Ç–æ—á–Ω–∏–∫–∏ - –±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ —Ä–µ—à–∏—Ç–µ–ª—å–Ω–æ –≤—Å—ë. –ü—Ä–∏—á—ë–º –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–æ–∫—Å–∏, –Ω–æ —Ç–∞–∫–∂–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è MITM –ø–æ–¥–º–µ–Ω–æ–π —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤, –ø–æ—ç—Ç–æ–º—É —Å–∫–∞—á–∞—Ç—å —Å [Hugging Face/Unsloth AI](https://huggingface.co/unsloth) —è –Ω–∏—á–µ–≥–æ –Ω–µ –º–æ–≥—É, –ø–æ–ª—É—á–∞—è –æ—Ç–ª—É–ø 500. –ò–∑–¥–µ—Ä–∂–∫–∏ —Ä–∞–±–æ—Ç—ã –≤ –±–æ–ª—å—à–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –¢–∞–∫ —á—Ç–æ –∫–∞—á–∞—Ç—å —è –±—É–¥—É –¥–æ–º–∞. –ê –¥–æ–º–∞ —É –º–µ–Ω—è –≤—Å–µ–≥–æ-–Ω–∞–≤—Å–µ–≥–æ 100 –ú–±–∏—Ç (—Ö–æ—Ç—å –∏ –æ–ø—Ç–æ–≤–æ–ª–æ–∫–Ω–æ), —Ç–∞–∫ —á—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –Ω–µ –±—ã—Å—Ç—Ä—ã–π (–ø–ª–∞–Ω–∏—Ä—É–π—Ç–µ –∫–∞–∫ –º–∏–Ω–∏–º—É–º —Å—É—Ç–∫–∏).

–ü–æ—Å–∫–æ–ª—å–∫—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π, —Ç–æ —è —Ä–µ—à–∏–ª, —á—Ç–æ —è —Å–∞–º—ã–π —É–º–Ω—ã–π, –∏ —Å–ª–µ–≥–∫–∞ –ø–æ–≥—É–≥–ª–∏–≤ –Ω–∞—à—ë–ª —Å–ø–æ—Å–æ–± –≤—ã–¥–µ—Ä–Ω—É—Ç—å —á–µ—Ä–µ–∑ –≥–∏—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É, –∞ –Ω–µ –≤—Å—é —Ä–µ–ø—É.

–í–æ—Ç —Ç–∞–∫ —ç—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è:

–Ø –≤–∫–ª—é—á–∏–ª —Å—Ç–∞—Ä—ã–π –∫–æ–º–ø —Å —Ñ–∞–π–ª–æ–ø–æ–º–æ–π–∫–æ–π, –ø—Ä–æ–≤–µ—Ä–∏–ª —Ç–∞–º —Å–≤–æ–±–æ–¥–Ω–æ–µ –º–µ—Å—Ç–æ, –≤—ã–ø–æ–ª–Ω–∏–ª –∫–æ–º–∞–Ω–¥—ã –≤—ã—à–µ –∏ –ø–æ—à—ë–ª —Å–ø–∞—Ç—å. –ù–∞—É—Ç—Ä–æ —è –æ–±–Ω–∞—Ä—É–∂–∏–ª, —á—Ç–æ —Ç—É–¥–∞ –ø—Ä–∏–ª–µ—Ç–µ–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏ –æ–Ω –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏–ª—Å—è. –ù—É, –±—ã–≤–∞–µ—Ç, —è –∏ –∑–∞–±—ã–ª, —á—Ç–æ –Ω–µ–∑–∞–¥–æ–ª–≥–æ –¥–æ —ç—Ç–æ–≥–æ –ø–æ—Å—Ç–∞–≤–∏–ª —Ç—É–¥–∞ —Å–≤–µ–∂—É—é Windows –¥–∞ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–∏–ª –∫–∞–∫ —Å–ª–µ–¥—É–µ—Ç.

–õ–∏—Ä–∏—á–µ—Å–∫–æ–µ –æ—Ç—Å—Ç—É–ø–ª–µ–Ω–∏–µ ‚Äî –µ—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–∞–∑–∞—Ç—å—Å—è –æ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –∏ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å–æ–ø—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–æ–∫, —Ç–æ –ø—Ä–æ—Å—Ç–æ –≤–∫–ª—é—á–∏—Ç–µ Metered connection (–ø–æ-–Ω–µ–º–µ—Ü–∫–∏ Getaktete Verbindung) –¥–ª—è —Å–µ—Ç–µ–≤–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞, —ç—Ç–æ —Å–∞–º—ã–π –Ω–∞–∏–ø—Ä–æ—Å—Ç–µ–π—à–∏–π —Å–ø–æ—Å–æ–±.

–Ø —ç—Ç–æ —Å–¥–µ–ª–∞–ª, –∏ –≤—ã–ø–æ–ª–Ω–∏–≤ –∫–æ–º–∞–Ω–¥—ã –≤—Ç–æ—Ä–æ–π —Ä–∞–∑, –æ–±–Ω–∞—Ä—É–∂–∏–ª, —á—Ç–æ "–¥–æ–∫–∞—á–∫–∏" —Ç–∞–º –Ω–µ—Ç –∏ –≤ –ø–æ–º–∏–Ω–µ, git –Ω–∞—á–∞–ª –∫–∞—á–∞—Ç—å —Å –Ω—É–ª—è (—ç—Ç–æ –≤–∏–¥–Ω–æ –≤ lfs –∫–µ—à–µ, —Ç–∞–º —Å–Ω–∞—á–∞–ª–∞ —Ñ–∞–π–ª—ã —Å–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É). –¢—É—Ç —è –≤—Å–ø–æ–º–Ω–∏–ª, —á—Ç–æ —É –º–µ–Ω—è –≤–∞–ª—è–µ—Ç—Å—è –¥—Ä–µ–≤–Ω–∏–π QNAP NAS, —Ç–∞–º –µ—Å—Ç—å –∫–∞—á–∞–ª–∫–∞, –Ω–æ —Ç—É—Ç –º–µ–Ω—è –∂–¥–∞–ª–∞ –¥—Ä—É–≥–∞—è –∑–∞—Å–∞–¥–∞ ‚Äî —Ä–∞–∑ –≤ —Å—É—Ç–∫–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Å–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ, –ø—Ä–∏ —ç—Ç–æ–º NAS –≤—ã–≤–∞–ª–∏–≤–∞–ª –æ—à–∏–±–∫—É –∏ –Ω–∞—á–∏–Ω–∞–ª —Å–∫–∞—á–∫—É —Å–Ω–æ–≤–∞. –° —Ç–æ—Ä—Ä–µ–Ω—Ç–∞–º–∏ –æ–Ω —Ö—É–¥–æ-–±–µ–¥–Ω–æ —Å–ø—Ä–∞–≤–ª—è–ª—Å—è, –∞ –≤–æ—Ç —Å https ‚Äî –Ω–µ—Ç. –Ø —É–∂–µ —Ö–æ—Ç–µ–ª –±—ã–ª–æ –æ—Ç–∫–∞–∑–∞—Ç—å—Å—è –æ—Ç –∑–∞—Ç–µ–∏, –Ω–æ –≤—ã –∂–µ –ø–æ–Ω–∏–º–∞–µ—Ç–µ ‚Äî –µ—Å–ª–∏ —è –≤–æ –≤—Å–µ—É—Å–ª—ã—à–∞–Ω–∏–µ –Ω–∞–ø–∏—à—É –∑–¥–µ—Å—å, —á—Ç–æ –Ω–µ —Å–º–æ–≥ —Å–∫–∞—á–∞—Ç—å —à–µ—Å—Ç—å—Å–æ—Ç –≥–∏–≥, —Ç–æ –Ω–∞–¥–æ –º–Ω–æ–π –±—É–¥–µ—Ç —Ä–∂–∞—Ç—å –≤–µ—Å—å —Ö–∞–±—Ä, –∏ —è –±—É–¥—É –ø–æ–¥–≤–µ—Ä–≥–Ω—É—Ç –ø—É–±–ª–∏—á–Ω–æ–º—É –æ—Å—Ç—Ä–∞–∫–∏–∑–º—É, —Ç–∞–∫ —á—Ç–æ –ø—Ä–∏—à–ª–æ—Å—å —Ä–∞—Å—á–µ—Ö–ª–∏—Ç—å  –º–µ–Ω–µ–¥–∂–µ—Ä –∑–∞–∫–∞—á–µ–∫, –∫–æ—Ç–æ—Ä—ã–º —è –Ω–µ –ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è —É–∂–µ –º–Ω–æ–≥–æ –ª–µ—Ç.

–Ø —ç—Ç–æ –≤—Å—ë –∫ —Ç–æ–º—É, —á—Ç–æ –µ—Å–ª–∏ –∑–∞—Ö–æ—Ç–∏—Ç–µ —Å–∫–∞—á–∞—Ç—å –Ω–∞ —Å–ª–∞–±–æ–º –∫–∞–Ω–∞–ª–µ ‚Äî –∑–∞—Ä–∞–Ω–µ–µ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –≤—ã –º–æ–∂–µ—Ç–µ —É–≤–µ—Ä–µ–Ω–Ω–æ –¥–æ–∫–∞—á–∏–≤–∞—Ç—å –ø—Ä–∏ –æ–±—Ä—ã–≤–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è.

–ö–∞–∫ –±—ã —Ç–æ –Ω–∏ –±—ã–ª–æ, –≤–æ—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π [DeepSeek-R1-GGUF](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main), –≤–æ—Ç –º–æ–¥–µ–ª—å [DeepSeek-R1-Q8_0](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-Q8_0), –∞ –ø–æ–¥ —Å–ø–æ–π–ª–µ—Ä–æ–º - –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ —Å–∫–æ—Ä–º–∏—Ç—å –ª—é–±–æ–º—É –º–µ–Ω–µ–¥–∂–µ—Ä—É, —è –ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è [JDownloader 2](https://jdownloader.org/download/index).

<spoiler title="DeepSeek-R1.Q8_0-000??-of-00015.gguf">

```

https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00001-of-00015.gguf?download=true
https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00002-of-00015.gguf?download=true
https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00003-of-00015.gguf?download=true
https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00004-of-00015.gguf?download=true
https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00005-of-00015.gguf?download=true
https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00006-of-00015.gguf?download=true
https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00007-of-00015.gguf?download=true
https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00008-of-00015.gguf?download=true
https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00009-of-00015.gguf?download=true
https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00010-of-00015.gguf?download=true
https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00011-of-00015.gguf?download=true
https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00012-of-00015.gguf?download=true
https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00013-of-00015.gguf?download=true
https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00014-of-00015.gguf?download=true
https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00015-of-00015.gguf?download=true

```

</spoiler>

–ü–ª—é—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –µ—â—ë –∏ –≤ —Ç–æ–º, —á—Ç–æ –¥–Ω—ë–º —è –º–æ–≥—É —É–º–µ–Ω—å—à–∏—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å, —á—Ç–æ–±—ã –Ω–µ –¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –Ω–µ—É–¥–æ–±—Å—Ç–≤ –¥–æ–º–∞—à–Ω–∏–º, –∞ –Ω–æ—á—å—é "–æ—Ç–∫—Ä—ã–≤–∞—Ç—å –∫—Ä–∞–Ω" –Ω–∞ –≤—Å—é –∫–∞—Ç—É—à–∫—É.

–ü–æ–∫–∞ –∏–¥—ë—Ç —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ, —á—Ç–æ–± –±—ã–ª–æ –Ω–µ —Å–∫—É—á–Ω–æ ‚Äî –º–æ–≥—É –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∑–∞–≥–∞–¥–∫—É.

–í–æ –≤—Ç–æ—Ä–æ–π –ø–æ–ª–æ–≤–∏–Ω–µ –¥–µ–≤—è–Ω–æ—Å—Ç—ã—Ö, –º—ã, –Ω–æ–≤–æ–∏—Å–ø–µ—á—ë–Ω–Ω—ã–µ –≤—ã–ø—É—Å–∫–Ω–∏–∫–∏ ‚Äî —Ñ–∏–∑–∏–∫–∏, –∏–Ω–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞–ª–∏—Å—å –≤–º–µ—Å—Ç–µ, —Ä–∞—Å–ø–∏–≤–∞–ª–∏ –∫—Ä–µ–ø–∫–∏–µ —Å–ø–∏—Ä—Ç–Ω—ã–µ –Ω–∞–ø–∏—Ç–∫–∏ –∏ –∏–Ω–æ–≥–¥–∞ —Å–º–æ—Ç—Ä–µ–ª–∏ —Ñ–∏–ª—å–º—ã –Ω–∞ –≤–∏–¥–µ–æ–∫–∞—Å—Å–µ—Ç–∞—Ö. –í —á–∏—Å–ª–µ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –±—ã–ª —Ñ–∏–ª—å–º, –∫–æ—Ç–æ—Ä—ã–π –º—ã, –ø–æ–¥—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–∞–º–∏, –ª–∞—Å–∫–æ–≤–æ –Ω–∞–∑—ã–≤–∞–ª–∏ –Ω–µ –∏–Ω–∞—á–µ –∫–∞–∫ "—ç—Ç–æ –Ω–∞—à–µ –∫–∏–Ω–æ, –ø—Ä–æ —ç—Ç–∏, –ø—Ä–æ –ì–ò–ì–ê–ë–ê–ô–¢–´!" –£–≥–∞–¥–∞–π—Ç–µ, –æ –∫–∞–∫–æ–º —Ñ–∏–ª—å–º–µ, –≤—ã–ø—É—â–µ–Ω–Ω–æ–º –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –¥–µ–≤—è–Ω–æ—Å—Ç—ã—Ö, —à–ª–∞ —Ä–µ—á—å? –ü–æ—Ç–æ–º —Å–ø—Ä–æ—Å–∏–º —É DeepSeek.

–ö–∞–∫ –±—ã —Ç–æ –Ω–∏ –±—ã–ª–æ, —á–µ—Ä–µ–∑ –ø–∞—Ä—É –¥–Ω–µ–π –∏ –Ω–æ—á–µ–π –Ω–∞ —Ç–µ—Ä–∞–±–∞–π—Ç–Ω–æ–º –¥–∏—Å–∫–µ —É –º–µ–Ω—è –ª–µ–∂–∞–ª–∏ –ø—è—Ç–Ω–∞–¥—Ü–∞—Ç—å –∑–∞–≤–µ—Ç–Ω—ã—Ö GGUF —Ñ–∞–π–ª–æ–≤. –ë–µ—Ä–µ–∂–Ω–æ –Ω–µ—Å—è –∏—Ö –Ω–∞ —Ä–∞–±–æ—Ç—É, –∏—Å–ø—ã—Ç–∞–ª —Å—Ç—Ä–∞–Ω–Ω–æ–µ –æ—â—É—â–µ–Ω–∏–µ ‚Äî —ç—Ç–æ —á—Ç–æ –∂–µ, —É –º–µ–Ω—è –≤ —Ä—É–∫–∞—Ö –ø–æ—á—Ç–∏ –≤—Å—è –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–≤–∏–Ω—Ç—ç—Å—Å–µ–Ω—Ü–∏—è –∑–Ω–∞–Ω–∏–π —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–∞?! –ú–¥–∞.

## –ó–∞–ø—É—Å–∫–∞–µ–º.

–í–æ–æ–±—â–µ –≥–æ–≤–æ—Ä—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–æ—Å–æ–±–æ–≤ "–∑–∞–ø—É—Å—Ç–∏—Ç—å" DeepSeek –ª–æ–∫–∞–ª—å–Ω–æ ‚Äî LM Studio, Ollama, llama.ccp –∏ OpenVINO, —á–∞—Å—Ç—å –∏–∑ –Ω–∏—Ö —è –ø–æ—á–µ—Ä–ø–Ω—É–ª –∏–∑ –∫–æ–º–º–µ–Ω—Ç–æ–≤, –∑–∞ —á—Ç–æ —Å–ø–∞—Å–∏–±–æ. –Ø –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª –Ω–∞–≤—Å–∫–∏–¥–∫—É –≤—Å–µ, –Ω–æ –±–µ–∑ —Ñ–∞–Ω–∞—Ç–∏–∑–º–∞.

–°–∞–º—ã–π –Ω–∞–∏–ø—Ä–æ—Å—Ç–µ–π—à–∏–π, –≤–µ—Ä–æ—è—Ç–Ω–æ ‚Äî [LM Studio](https://lmstudio.ai). –ê–∫—Ç—É–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –Ω–∞ –º–æ–º–µ–Ω—Ç –Ω–∞–ø–∏—Å–∞–Ω–∏—è –ø–æ—Å—Ç–∞ ‚Äî [0.3.9](https://installers.lmstudio.ai/win32/x64/0.3.9-6/LM-Studio-0.3.9-6-x64.exe). –Ø –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª –ø–æ—Å—Ç–∞–≤–∏—Ç—å –¥–æ–º–∞, –≤—Å—ë –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –∫–∞–∫ –¥–µ–º–∫–∞ —Å–∫–∞—á–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–µ–Ω—å–∫–∞—è [DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF) –Ω–∞ 4 —Å –ø–æ–ª–æ–≤–∏–Ω–æ–π –≥–∏–≥–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–ø—É—Å—Ç–∏—Ç—å —ç—Ç–æ –¥–µ–ª–æ –¥–∞–∂–µ –Ω–∞ –º–æ—ë–º –¥—Ä–µ–≤–Ω–µ–º –Ω–æ—É—Ç–µ (32 –ì–ë, –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä i7-4940MX). –Ø –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª ‚Äî –¥–∞–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ. –§–∞–π–ª—ã –º–æ–¥–µ–ª–µ–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ª–µ–∂–∞—Ç –≤ %USERPROFILE%\\.lmstudio\models\lmstudio-community\, —Ç–∞–º –æ–Ω–∏ –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö —Ç–∏–ø–∞ \\DeepSeek-R1-Distill-Qwen-7B-GGUF.

–Ø –∑–∞–∫–∏–Ω—É–ª –Ω–∞ –¥–∏—Å–∫ –∏–Ω—Å—Ç–∞–ª–ª—è—Ç–æ—Ä, –Ω–æ –≤–æ—Ç –Ω–∞ hp z8 –ø—Ä–∏–ª–µ—Ç–µ–ª–∞ –∏ —É—Å–µ–ª–∞—Å—å –ø—Ç–∏—á–∫–∞ –æ–±–ª–æ–º–∏–Ω–≥–æ ‚Äî "—Ç–µ—Å—Ç–æ–≤–∞—è" Qwen 7B —Ö–æ—Ç—å –∏ –∑–∞–≥—Ä—É–∂–∞–ª–∞—Å—å, –Ω–æ –≤—ã–≤–∞–ª–∏–≤–∞–ª–∞—Å—å –≤ "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—É—é –æ—à–∏–±–∫—É" –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –∂–µ –ø—Ä–æ–º–ø—Ç–∞, –∞ —Å —Ç–∞–∫–∏–º —Ç—Ä—É–¥–æ–º –≤—ã–∫–∞—á–∞–Ω–Ω–∞—è Q8_0 –æ—Ç–∫–∞–∑–∞–ª–∞—Å—å –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è –≤–æ–≤—Å–µ, —Å–æ—Å–ª–∞–≤—à–∏—Å—å –Ω–∞ –Ω–µ—Ö–≤–∞—Ç–∫—É —Ä–µ—Å—É—Ä—Å–æ–≤ (–≤—ã–≥—Ä—É–∑–∫—É –≤ GPU —è –æ—Ç–∫–ª—é—á–∏–ª, –Ω–µ –ø–æ–º–æ–≥–ª–æ). –î–∞–ª—å—à–µ —è –º—É—á–∏—Ç—å –µ—ë –ø–æ–∫–∞ –Ω–µ —Å—Ç–∞–ª –∏ –æ—Ç–ª–æ–∂–∏–ª –∫—Ä–∞—Å–∏–≤—É—é –∏–≥—Ä—É—à–∫—É –≤ —Å—Ç–æ—Ä–æ–Ω—É.

[Ollama](https://ollama.com) –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞ –Ω–∞ —Ç–æ, —á—Ç–æ —Å–∫–∞—á–∏–≤–∞—Ç—å –º–æ–¥–µ–ª—å –≤—ã –±—É–¥–µ—Ç–µ —á–µ—Ä–µ–∑ –Ω–µ—ë –∂–µ (—á—Ç–æ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤ –º–æ—ë–º —Å–ª—É—á–∞–µ, –∞ –ø–µ—Ä–µ–∫–∞—á–∏–≤–∞—Ç—å –¥–æ–º–∞ –≤—Å—ë –µ—â—ë —Ä–∞–∑ ‚Äî —Ç—É—Ç —É–∂ –∏–∑–≤–∏–Ω–∏—Ç–µ), 

```
Welcome to Ollama!
Run your first model:

        ollama run llama3.2

PS C:\Windows\System32> ollama run deepseek-r1:1.5b
pulling manifest
pulling aabd4debf0c8... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.1 GB
pulling 369ca498f347... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  387 B
pulling 6e4c38e1172f... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.1 KB
verifying sha256 digest, writing manifest, success
>>> hi!
<think>

</think>

Hello! How can I assist you today? üòä
```
—Ç–∞–∫ —Ç–æ –æ–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∞ –≤–æ—Ç –∫–∞–∫ "–ø–æ–¥–æ—Ç–∫–Ω—É—Ç—å" –µ–π —É–∂–µ –∑–∞—Ä–∞–Ω–µ–µ —Å–∫–∞—á–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã GGUF ‚Äî —Ç—É—Ç —á—É—Ç—å —Å–ª–æ–∂–Ω–µ–µ. –¢–æ –µ—Å—Ç—å –º–µ—Å—Ç–æ-—Ç–æ –≥–¥–µ –æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –ª–µ–∂–∞—Ç—å —è –∑–Ω–∞—é: %USERPROFILE%\\.ollama\models, –Ω–æ —Ç–∞–º –º–æ–¥–µ–ª—å –Ω–∞–¥–æ –∫–∏–¥–∞—Ç—å –≤ –ø–∞–ø–∫—É \blobs, –ø—Ä–∏ —ç—Ç–æ–º –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å —Ñ–∞–π–ª –≤ sha256-<—Ö–µ—à>, –∫—Ä–æ–º–µ —Ç–æ–≥–æ,  –Ω–∞–¥–æ –¥–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ \manifests\registry\\.ollama.ai\library\deepseek-r1, –≤ –æ–±—â–µ–º —Ç—É—Ç –Ω–∞–¥–æ "–ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å" –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ 
"ollama create <your-model-name-here> -f DeepSeek-R1.gguf", –Ω–æ –ø–µ—Ä–µ–¥ —ç—Ç–∏–º –≤–∞–º –ø—Ä–∏–¥—ë—Ç—Å—è —Å–º–µ—Ä–∂–∏—Ç—å –≤—Å–µ –ø—è—Ç–Ω–∞–¥—Ü–∞—Ç—å —Ñ–∞–π–ª–æ–≤ –≤–º–µ—Å—Ç–µ —á–µ—Ä–µ–∑ "gguf-split --merge DeepSeek-R1.Q8_0-00001-of-00015.gguf DeepSeek-R1.gguf", –∏ –≤ –æ–±—â–µ–º –Ω—É –µ–≥–æ –≤ —Ç–æ–ø–∫—É —Ç–∞–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è.

–ê –≤–æ—Ç —Å llama.ccp –≤—Å—ë –ø–æ–ª—É—á–∏–ª–æ—Å—å, –∏ –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ. –Ø –±—ã–ª–æ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏–ª—Å—è –¥–æ–ª–≥–æ –∏ –Ω—É–¥–Ω–æ —Å–æ–±–∏—Ä–∞—Ç—å —ç—Ç–æ –¥–µ–ª–æ –∏–∑ –∏—Å—Ö–æ–¥–Ω–∏–∫–æ–≤, –Ω–æ –¥–µ–ª–∞—Ç—å —ç—Ç–æ –Ω–µ –Ω–∞–¥–æ, —Ç–∞–º –µ—Å—Ç—å –≥–æ—Ç–æ–≤—ã–µ —Å–±–æ—Ä–∫–∏ –ø–æ–¥ Windows. –ü–æ—Å–∫–æ–ª—å–∫—É —É –º–µ–Ω—è –∂–µ–ª–µ–∑–∫–∞ –Ω–∞ Xeon —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π AVX512, –≤–æ—Ç —ç—Ç—É –≤–µ—Ä—Å–∏—é —è –∏ —Å–∫–∞—á–∞–ª –¥–ª—è –Ω–∞—á–∞–ª–∞.

–ö–æ—Ä–æ—á–µ, —Ç—É–ø–æ –∑–∞–±—Ä–∞—Å—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∞—Ä—Ö–∏–≤–∞ —Å —É—Ç–∏–ª–∏—Ç–∞–º–∏ –≤ –æ–¥–Ω—É –ø–∞–ø–∫—É (–ø—Ä—è–º–æ –≤ –∫–æ—Ä–µ–Ω—å –¥–∏—Å–∫–∞ F:\ –≤ –º–æ—ë–º —Å–ª—É—á–∞–µ)
–í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ (–Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, –ø–æ–¥ —Å–ø–æ–π–ª–µ—Ä–æ–º). –∏—Ö —Ç–∞–º –æ–≤–µ—Ä–¥–æ—Ñ–∏–≥–∞:

<spoiler title="llama-cli.exe -h">

```
example usage:

  text generation:     llama-cli.exe -m your_model.gguf -p "Hi!" -n 128
  chat (conversation): llama-cli.exe -m your_model.gguf -p "Hello!" -cnv

----- common params -----

-h,    --help, --usage                  print usage and exit
--version                               show version and build info
--verbose-prompt                        print a verbose prompt before generation (default: false)
-t,    --threads N                      number of threads to use during generation (default: -1)
                                        (env: LLAMA_ARG_THREADS)
-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:
                                        same as --threads)
-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range
                                        (default: "")
-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask
--cpu-strict <0|1>                      use strict CPU placement (default: 0)
--prio N                                set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime
                                        (default: 0)
--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)
-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch
                                        (default: same as --cpu-mask)
-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch
--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)
--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime
                                        (default: 0)
--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)
-c,    --ctx-size N                     size of the prompt context (default: 4096, 0 = loaded from model)
                                        (env: LLAMA_ARG_CTX_SIZE)
-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity, -2 = until
                                        context filled)
                                        (env: LLAMA_ARG_N_PREDICT)
-b,    --batch-size N                   logical maximum batch size (default: 2048)
                                        (env: LLAMA_ARG_BATCH)
-ub,   --ubatch-size N                  physical maximum batch size (default: 512)
                                        (env: LLAMA_ARG_UBATCH)
--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =
                                        all)
-fa,   --flash-attn                     enable Flash Attention (default: disabled)
                                        (env: LLAMA_ARG_FLASH_ATTN)
-p,    --prompt PROMPT                  prompt to start generation with
                                        if -cnv is set, this will be used as system prompt
--no-perf                               disable internal libllama performance timings (default: false)
                                        (env: LLAMA_ARG_NO_PERF)
-f,    --file FNAME                     a file containing the prompt (default: none)
-bf,   --binary-file FNAME              binary file containing the prompt (default: none)
-e,    --escape                         process escapes sequences (\n, \r, \t, \', \", \\) (default: true)
--no-escape                             do not process escape sequences
--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by
                                        the model
                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)
--rope-scale N                          RoPE context scaling factor, expands context by a factor of N
                                        (env: LLAMA_ARG_ROPE_SCALE)
--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from
                                        model)
                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)
--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N
                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)
--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training
                                        context size)
                                        (env: LLAMA_ARG_YARN_ORIG_CTX)
--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full
                                        interpolation)
                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)
--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)
                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)
--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)
                                        (env: LLAMA_ARG_YARN_BETA_SLOW)
--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)
                                        (env: LLAMA_ARG_YARN_BETA_FAST)
-dkvc, --dump-kv-cache                  verbose print of the KV cache
-nkvo, --no-kv-offload                  disable KV offload
                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)
-ctk,  --cache-type-k TYPE              KV cache data type for K
                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1
                                        (default: f16)
                                        (env: LLAMA_ARG_CACHE_TYPE_K)
-ctv,  --cache-type-v TYPE              KV cache data type for V
                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1
                                        (default: f16)
                                        (env: LLAMA_ARG_CACHE_TYPE_V)
-dt,   --defrag-thold N                 KV cache defragmentation threshold (default: 0.1, < 0 - disabled)
                                        (env: LLAMA_ARG_DEFRAG_THOLD)
-np,   --parallel N                     number of parallel sequences to decode (default: 1)
                                        (env: LLAMA_ARG_N_PARALLEL)
--rpc SERVERS                           comma separated list of RPC servers
                                        (env: LLAMA_ARG_RPC)
--mlock                                 force system to keep model in RAM rather than swapping or compressing
                                        (env: LLAMA_ARG_MLOCK)
--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not
                                        using mlock)
                                        (env: LLAMA_ARG_NO_MMAP)
--numa TYPE                             attempt optimizations that help on some NUMA systems
                                        - distribute: spread execution evenly over all nodes
                                        - isolate: only spawn threads on CPUs on the node that execution
                                        started on
                                        - numactl: use the CPU map provided by numactl
                                        if run without this previously, it is recommended to drop the system
                                        page cache before using this
                                        see https://github.com/ggerganov/llama.cpp/issues/1437
                                        (env: LLAMA_ARG_NUMA)
-dev,  --device <dev1,dev2,..>          comma-separated list of devices to use for offloading (none = don't
                                        offload)
                                        use --list-devices to see a list of available devices
                                        (env: LLAMA_ARG_DEVICE)
--list-devices                          print list of available devices and exit
-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM
                                        (env: LLAMA_ARG_N_GPU_LAYERS)
-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:
                                        - none: use one GPU only
                                        - layer (default): split layers and KV across GPUs
                                        - row: split rows across GPUs
                                        (env: LLAMA_ARG_SPLIT_MODE)
-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of
                                        proportions, e.g. 3,1
                                        (env: LLAMA_ARG_TENSOR_SPLIT)
-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for
                                        intermediate results and KV (with split-mode = row) (default: 0)
                                        (env: LLAMA_ARG_MAIN_GPU)
--check-tensors                         check model tensor data for invalid values (default: false)
--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified
                                        multiple times.
                                        types: int, float, bool, str. example: --override-kv
                                        tokenizer.ggml.add_bos_token=bool:false
--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)
--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use
                                        multiple adapters)
--control-vector FNAME                  add a control vector
                                        note: this argument can be repeated to add multiple control vectors
--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE
                                        note: this argument can be repeated to add multiple scaled control
                                        vectors
--control-vector-layer-range START END
                                        layer range to apply the control vector(s) to, start and end inclusive
-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`
                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)
                                        (env: LLAMA_ARG_MODEL)
-mu,   --model-url MODEL_URL            model download url (default: unused)
                                        (env: LLAMA_ARG_MODEL_URL)
-hf,   -hfr, --hf-repo <user>/<model>[:quant]
                                        Hugging Face model repository; quant is optional, case-insensitive,
                                        default to Q4_K_M, or falls back to the first file in the repo if
                                        Q4_K_M doesn't exist.
                                        example: unsloth/phi-4-GGUF:q4_k_m
                                        (default: unused)
                                        (env: LLAMA_ARG_HF_REPO)
-hfd,  -hfrd, --hf-repo-draft <user>/<model>[:quant]
                                        Same as --hf-repo, but for the draft model (default: unused)
                                        (env: LLAMA_ARG_HFD_REPO)
-hff,  --hf-file FILE                   Hugging Face model file. If specified, it will override the quant in
                                        --hf-repo (default: unused)
                                        (env: LLAMA_ARG_HF_FILE)
-hfv,  -hfrv, --hf-repo-v <user>/<model>[:quant]
                                        Hugging Face model repository for the vocoder model (default: unused)
                                        (env: LLAMA_ARG_HF_REPO_V)
-hffv, --hf-file-v FILE                 Hugging Face model file for the vocoder model (default: unused)
                                        (env: LLAMA_ARG_HF_FILE_V)
-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment
                                        variable)
                                        (env: HF_TOKEN)
--log-disable                           Log disable
--log-file FNAME                        Log to file
--log-colors                            Enable colored logging
                                        (env: LLAMA_LOG_COLORS)
-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for
                                        debugging)
-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be
                                        ignored.
                                        (env: LLAMA_LOG_VERBOSITY)
--log-prefix                            Enable prefx in log messages
                                        (env: LLAMA_LOG_PREFIX)
--log-timestamps                        Enable timestamps in log messages
                                        (env: LLAMA_LOG_TIMESTAMPS)


----- sampling params -----

--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by
                                        ';'
                                        (default: penalties;dry;top_k;typ_p;top_p;min_p;xtc;temperature)
-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)
--sampling-seq, --sampler-seq SEQUENCE
                                        simplified sequence for samplers that will be used (default: edkypmxt)
--ignore-eos                            ignore end of stream token and continue generating (implies
                                        --logit-bias EOS-inf)
--temp N                                temperature (default: 0.8)
--top-k N                               top-k sampling (default: 40, 0 = disabled)
--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)
--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)
--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)
--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)
--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)
--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1
                                        = ctx_size)
--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)
--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)
--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)
--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)
--dry-base N                            set DRY sampling base value (default: 1.75)
--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)
--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =
                                        context size)
--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers
                                        ('\n', ':', '"', '*') in the process; use "none" to not use any
                                        sequence breakers
--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)
--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)
--mirostat N                            use Mirostat sampling.
                                        Top K, Nucleus and Locally Typical samplers are ignored if used.
                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)
--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)
--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)
-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,
                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',
                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'
--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/
                                        dir) (default: '')
--grammar-file FNAME                    file to read grammar from
-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.
                                        `{}` for any JSON object
                                        For schemas w/ external $refs, use --grammar +
                                        example/json_schema_to_grammar.py instead


----- example-specific params -----

--no-display-prompt                     don't print prompt at generation (default: false)
-co,   --color                          colorise output to distinguish prompt and user input from generations
                                        (default: false)
--no-context-shift                      disables context shift on inifinite text generation (default:
                                        disabled)
                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)
-ptc,  --print-token-count N            print token count every N tokens (default: -1)
--prompt-cache FNAME                    file to cache prompt state for faster startup (default: none)
--prompt-cache-all                      if specified, saves user input and generations to cache as well
--prompt-cache-ro                       if specified, uses the prompt cache but does not update it
-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode
-sp,   --special                        special tokens output enabled (default: false)
-cnv,  --conversation                   run in conversation mode:
                                        - does not print special tokens and suffix/prefix
                                        - interactive mode is also enabled
                                        (default: auto enabled if chat template is available)
-no-cnv, --no-conversation              force disable conversation mode (default: false)
-i,    --interactive                    run in interactive mode (default: false)
-if,   --interactive-first              run in interactive mode and wait for input right away (default: false)
-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\'
--in-prefix-bos                         prefix BOS to user inputs, preceding the `--in-prefix` string
--in-prefix STRING                      string to prefix user inputs with (default: empty)
--in-suffix STRING                      string to suffix after user inputs with (default: empty)
--no-warmup                             skip warming up the model with an empty run
-gan,  --grp-attn-n N                   group-attention factor (default: 1)
                                        (env: LLAMA_ARG_GRP_ATTN_N)
-gaw,  --grp-attn-w N                   group-attention width (default: 512)
                                        (env: LLAMA_ARG_GRP_ATTN_W)
--jinja                                 use jinja template for chat (default: disabled)
                                        (env: LLAMA_ARG_JINJA)
--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's
                                        metadata)
                                        if suffix/prefix are specified, template will be disabled
                                        only commonly used templates are accepted (unless --jinja is set
                                        before this flag):
                                        list of built-in templates:
                                        chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2, deepseek3,
                                        exaone3, falcon3, gemma, gigachat, glmedge, granite, llama2,
                                        llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, megrez, minicpm,
                                        mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7, monarch,
                                        openchat, orion, phi3, phi4, rwkv-world, vicuna, vicuna-orca, zephyr
                                        (env: LLAMA_ARG_CHAT_TEMPLATE)
--chat-template-file JINJA_TEMPLATE_FILE
                                        set custom jinja chat template file (default: template taken from
                                        model's metadata)
                                        if suffix/prefix are specified, template will be disabled
                                        only commonly used templates are accepted (unless --jinja is set
                                        before this flag):
                                        list of built-in templates:
                                        chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2, deepseek3,
                                        exaone3, falcon3, gemma, gigachat, glmedge, granite, llama2,
                                        llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, megrez, minicpm,
                                        mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7, monarch,
                                        openchat, orion, phi3, phi4, rwkv-world, vicuna, vicuna-orca, zephyr
                                        (env: LLAMA_ARG_CHAT_TEMPLATE_FILE)
--simple-io                             use basic IO for better compatibility in subprocesses and limited
                                        consoles

example usage:

  text generation:     llama-cli.exe -m your_model.gguf -p "I believe the meaning of life is" -n 128

  chat (conversation): llama-cli.exe -m your_model.gguf -p "You are a helpful assistant" -cnv


```
</spoiler>

## "–†–∞–±–æ—Ç–∞–µ–º"

–ù—É –≤–æ—Ç –ø—Ä—è–º "—Ä–∞–±–æ—Ç–æ–π" —ç—Ç–æ –Ω–∞–∑–≤–∞—Ç—å —Å–ª–æ–∂–Ω–æ, –ø–æ—Ç–æ–º—É —á—Ç–æ —ç—Ç–æ –≤—Å—ë –æ—Ç—á–∞—è–Ω–Ω–æ –º–µ–¥–ª–µ–Ω–Ω–æ.

–°—Ä–∞–∑—É –ø–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª—å —Ü–µ–ª–∏–∫–æ–º –≤ –ø–∞–º—è—Ç—å –Ω–µ –≥—Ä—É–∑–∏—Ç—Å—è, –æ–Ω–∞ –∫–∞–∫ –±—ã "–¥–æ–≥—Ä—É–∂–∞–µ—Ç—Å—è" –ø—Ä–∏ –æ–±—â–µ–Ω–∏–∏.

–ü—Ä–∏ –ø–µ—Ä–≤–æ–º "—Ö–æ–ª–æ–¥–Ω–æ–º" —Å—Ç–∞—Ä—Ç–µ –¥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è –ø—Ä–∏–≥–ª–∞—à–µ–Ω–∏—è –ø—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ—á—Ç–∏ –¥–≤–µ –º–∏–Ω—É—Ç—ã.

–Ø —Å–º–∞—Å—Ç–µ—Ä–∏–ª —Ç–∞–∫—É—é –≤–æ—Ç –Ω–µ—Å–ª–æ–∂–Ω—É—é –æ—Å–Ω–∞—Å—Ç–∫—É, –ø–æ–∫–∞–∑—ã–≤–∞—é—â—É—é –∑–∞–≥—Ä—É–∑–∫—É –ø–∞–º—è—Ç–∏ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞, –Ω—É –∏ —Ç–∞–π–º–µ—Ä –¥–æ –∫—É—á–∏, —á—Ç–æ –± –≤—ã –ø—Ä–æ—á—É–≤—Å—Ç–≤–æ–≤–∞–ª–∏ —ç—Ç—É –±–æ–ª—å (—è –µ—â—ë –≤—ã—Ä–µ–∑–∞–ª –ø—Ä–∏–º–µ—Ä–Ω–æ –º–∏–Ω—É—Ç—É, –ø–æ–∫–∞ —á–∞—Å—Ç—å –º–æ–¥–µ–ª–∏ –≥—Ä—É–∑–∏–ª–∞—Å—å —Å –¥–∏—Å–∫–∞ –≤ –ø–∞–º—è—Ç—å, –æ—Å—Ç–∞–ª—å–Ω–æ–µ –æ—Å—Ç–∞–≤–ª—é –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –∫–∞–∫ –µ—Å—Ç—å):
![](https://habrastorage.org/webt/fw/dy/na/fwdyna4ul4ucut-zug3yhonhhwc.gif)
–ö–∞–∫ –≤–∏–¥–∏—Ç–µ, –ø–∞–º—è—Ç—å –∑–∞–ø–æ–ª–Ω–∏–ª–∞—Å—å –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ —Ç—Ä–µ—Ç—å –ø–æ—Å–ª–µ —Å—Ç–∞—Ä—Ç–∞ –∏ —á—É—Ç—å –±–æ–ª–µ–µ —á–µ–º –Ω–∞–ø–æ–ª–æ–≤–∏–Ω—É –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞.

–ï—Å–ª–∏ –≤–≤–æ–¥–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã, —Ç–æ –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –¥–æ–≥—Ä—É–∂–∞—Ç—å—Å—è. –ù–µ—Ç —Å–º—ã—Å–ª–∞ –º—É—á–∏—Ç—å –≤–∞—Å –º–µ–¥–ª–µ–Ω–Ω—ã–º –≤—ã–≤–æ–¥–æ–º, —è –±—É–¥—É —Å–Ω–∏–º–∞—Ç—å –∫–∞–¥—Ä –≤ —Å–µ–∫—É–Ω–¥—É, –ø—Ä–æ–∏–≥—Ä—ã–≤–∞—è –≥–∏—Ñ–∫—É —Å –ø—Ä–∏–º–µ—Ä–Ω–æ –¥–≤–∞–¥—Ü–∞—Ç–∏–∫—Ä–∞—Ç–Ω—ã–º —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º. –ü–æ–ø—Ä–æ—Å–∏–º —É –Ω–µ—ë [–†—É–Ω–≥–µ-–ö—É—Ç—Ç–∞](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%A0%D1%83%D0%BD%D0%B3%D0%B5_%E2%80%94_%D0%9A%D1%83%D1%82%D1%82%D1%8B), —Ö–æ—Ä–æ—à–æ –≤–∏–¥–Ω–æ, –∫–∞–∫ –æ–Ω–∞ –Ω–∞ —Ö–æ–¥—É –∑–∞–ø–æ–ª–Ω—è–µ—Ç —Å–µ–º—å—Å–æ—Ç –≥–∏–≥–∞–±–∞–π—Ç –ø–∞–º—è—Ç–∏ –ø–æ—á—Ç–∏ –ø–æ–¥ –∑–∞–≤—è–∑–∫—É:
![](https://habrastorage.org/webt/bv/w8/db/bvw8db0gki2rnbcr1k77z3lih-u.gif)

<spoiler title="Can you please provide C code of Runge Kutta Method for (x-y)/(x+y)?">

> Hi, DeepSeek!
> <think>

Hi! How can I assist you today?

> Can you please provide C code of Runge Kutta Method for (x-y)/(x+y)?
> <think>

Certainly! The Runge-Kutta method is a numerical technique used to solve ordinary differential equations (ODEs). For the differential equation dy/dx = (x - y)/(x + y), we can apply the 4th-order Runge-Kutta method (RK4). Here's a C code implementation:

```cpp
#include <stdio.h>
#include <math.h>

// Define the differential equation dy/dx = (x - y)/(x + y)
double f(double x, double y) {
    return (x - y) / (x + y);
}

// 4th-order Runge-Kutta method
void rungeKutta4(double x0, double y0, double h, double x_target) {
    double x = x0;
    double y = y0;
    double k1, k2, k3, k4;

    printf("Initial conditions: x = %lf, y = %lf\n", x, y);

    while (x < x_target) {
        if (x + h > x_target) {
            h = x_target - x; // Adjust step to reach x_target exactly
        }

        k1 = h * f(x, y);
        k2 = h * f(x + h/2, y + k1/2);
        k3 = h * f(x + h/2, y + k2/2);
        k4 = h * f(x + h, y + k3);

        y = y + (k1 + 2*k2 + 2*k3 + k4)/6;
        x = x + h;

        printf("x = %lf, y = %lf\n", x, y);
    }
}

int main() {
    double x0 = 1.0;   // Initial x
    double y0 = 0.0;   // Initial y
    double h = 0.1;    // Step size
    double x_target = 2.0; // Target x value

    rungeKutta4(x0, y0, h, x_target);
    return 0;
}
```

**Important Notes:**
1. **Initial Conditions:** The current example uses `x0=1.0`, `y0=0.0`, step size `h=0.1`, and integrates up to `x_target=2.0`. Adjust these values as needed.
2. **Division by Zero:** The function `f(x,y)` will have a division by zero if `x + y = 0`. Ensure your initial conditions and step size avoid this scenario.
3. **Adaptive Step Size:** For better accuracy, you might want to implement adaptive step size control, but this example uses a fixed step size for simplicity.
4. **Output:** The code prints the solution at each step. Modify the `printf` statements as needed for your use case.

Let me know if you need further clarification! 

>

</spoiler>
–ù–∞ –≤—Å—ë –ø—Ä–æ –≤—Å—ë —É –Ω–µ—ë —É—à–ª–æ –¥–µ—Å—è—Ç—å –º–∏–Ω—É—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ (–ø—Ä–æ–≤–µ—Ä–∫—É –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Ä–µ—à–µ–Ω–∏—è –æ—Å—Ç–∞–≤–∏–º –∑–∞ –∫–∞–¥—Ä–æ–º).
–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä, –∫–∞–∫ –≤–∏–¥–∏—Ç–µ, –±—ã–ª –∑–∞–Ω—è—Ç —á—É—Ç—å –±–æ–ª—å—à–µ —á–µ–º –Ω–∞ –ø–æ–ª–æ–≤–∏–Ω—É. –¢—É—Ç –Ω–∞–¥–æ –∑–∞–º–µ—Ç–∏—Ç—å, —á—Ç–æ —É –º–µ–Ω—è –±—ã–ª –≤–∫–ª—é—á—ë–Ω –≥–∏–≥–ø–µ—Ä—Ç—Ä–µ–¥–∏–Ω–≥, —Ç–∞–∫ —á—Ç–æ —Å–µ–≥–æ–¥–Ω—è —É—Ç—Ä–æ–º –ø–µ—Ä–≤—ã–º –¥–µ–ª–æ–º —è –µ–≥–æ –æ—Ç–∫–ª—é—á–∏–ª –∏ –ø–æ–≤—Ç–æ—Ä–∏–ª —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç, –∏ –¥–∞, –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –ù–¢ –∫–ª–∞–¥—ë—Ç —Å—Ç—Ä–µ–ª–∫—É –Ω–∞—à–µ–≥–æ —Å–ø–∏–¥–æ–º–µ—Ç—Ä–∞ –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª—å:

![](https://habrastorage.org/webt/ks/6q/mc/ks6qmc7be895eh3y01j0sblzryk.gif)

–ü–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Å–∏–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —è –Ω–µ –∑–∞–º–µ—Ç–∏–ª, –ø—Ä–∏–º–µ—Ä–Ω–æ —Ç–æ–∂–µ —Å–∞–º–æ–µ, —Ç–æ–∫–µ–Ω –≤ —Å–µ–∫—É–Ω–¥—É –∏–ª–∏ –æ–∫–æ–ª–æ —Ç–æ–≥–æ. –ù–æ –≥–æ—Ä–∞–∑–¥–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–µ–µ —Ç—É—Ç –¥—Ä—É–≥–æ–µ. –®—ë–ª –≤—Ç–æ—Ä–æ–π –¥–µ–Ω—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, —è –∏ –ø–æ–∂–µ–ª–∞–ª –º–æ–¥–µ–ª–∏ –¥–æ–±—Ä–æ–≥–æ —É—Ç—Ä–∞ (—è –æ–±—ã—á–Ω–æ –≤–µ–∂–ª–∏–≤—ã–π, –¥–∞). –û–Ω–∞ –æ—á–µ–Ω—å –æ–±—Ä–∞–¥–æ–≤–∞–ª–∞—Å—å, –ø—Ä–æ—Å–≤–µ—Ç–ª–µ–ª–∞ (–¥–∞–∂–µ –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ, —á—Ç–æ —è –æ—à–∏–±—Å—è –≤ –∏–º–µ–Ω–∏), –∏ –Ω–∞ –º–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å –ø—Ä–æ –†—É–Ω–≥–µ-–ö—É—Ç—Ç–∞ –ø—É—Å—Ç–∏–ª–∞—Å—å –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω–Ω—ã–µ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è ~~–æ —Å–º—ã—Å–ª–µ –∂–∏–∑–Ω–∏~~ –Ω–∞ —Å–æ—Ä–æ–∫ –º–∏–Ω—É—Ç (–≤—Ä–µ–º—è –≤ –≥–∏—Ñ–∫–µ –≤—ã—à–µ). 

–ò –≤–æ—Ç —Ç—É—Ç –∫–∞–∫ —Ä–∞–∑ –≤–æ–∑–Ω–∏–∫ —Ç–æ—Ç "–∞–≥–∞!" –º–æ–º–µ–Ω—Ç, –æ –∫–æ—Ç–æ—Ä–æ–º –º–Ω–æ–≥–æ –≥–¥–µ —É–ø–æ–º–∏–Ω–∞–ª–∏, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –≤—ã–¥–∞—ë—Ç —á—Ç–æ-—Ç–æ —Ç–∏–ø–∞ "Wait, but the step function can be part of the loop. Alternatively, write a function that takes x, y, h, and returns the new y..."

–í–æ–æ–±—â–µ —è —Å—á–∏—Ç–∞—é, —á—Ç–æ –≤—ã–¥–∞–Ω–Ω–∞—è –ø—Ä–æ—Å—Ç—ã–Ω—è –≤–ø–æ–ª–Ω–µ –∑–∞—Å–ª—É–∂–∏–≤–∞–µ—Ç —Ç–æ–≥–æ, —á—Ç–æ–±—ã –±—ã—Ç—å –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω–æ–π –ø–æ–ª–Ω–æ—Å—Ç—å—é.

<spoiler title="–û—á–µ–Ω—å –º–Ω–æ–≥–æ –±—É–∫–≤">


> Good morning, DeppSeek!

<think>
Okay, the user greeted me with "Good morning, DeppSeek!" Let me start by acknowledging their greeting and setting a friendly tone. Maybe I should respond with a cheerful good morning and ask how I can assist them today. Since they might need help with something specific, I should keep the response open-ended to encourage them to share their request. I'll make sure to keep it concise and welcoming.
</think>

Good morning! ÔøΩ How can I assist you today? Whether it's brainstorming ideas, solving
a problem, or just chatting, I'm here to help! Let me know what's on your mind. ÔøΩ

> Please provide C code for Runge Kutta method for (x-y)/(x+y)

<think>
Okay, the user is asking for C code that implements the Runge-Kutta method for the differential equation given by (x - y)/(x + y). Let me start by recalling what the Runge-Kutta method is. The fourth-order Runge-Kutta (RK4) method is a numerical technique used to solve ordinary differential equations (ODEs) with a given initial condition. It's more accurate than Euler's method because it uses multiple intermediate steps to approximate the solution over each interval.

First, I need to confirm the ODE the user wants to solve. The function provided is (x - y)/(x + y). Typically, ODEs are expressed as dy/dx = f(x, y), so I'll assume that the differential equation here is dy/dx = (x - y)/(x + y). That makes sense.

Next, I should outline the RK4 method. The RK4 method computes the next value y_{n+1} using the current value y_n and the weighted average of four increments:

k1 = h * f(x_n, y_n)
k2 = h * f(x_n + h/2, y_n + k1/2)
k3 = h * f(x_n + h/2, y_n + k2/2)
k4 = h * f(x_n + h, y_n + k3)

Then, y_{n+1} = y_n + (k1 + 2*k2 + 2*k3 + k4)/6

So, in the C code, I'll need to implement this formula.

The user wants a C program. Let me structure the code step by step. The code should include the necessary headers, define the function f(x, y) corresponding to the ODE, and implement the RK4 step as a function. Then, in the main function, set the initial conditions, step size h, number of steps, and loop through each step to compute the solution.


Wait, initial conditions are needed. Since the user didn't specify them, I should choose a default initial value, say x0, y0, and a range for x. Let's assume the user wants to start at some x0 (like 0) with y0, and compute up to a certain point. But since the user didn't specify, perhaps the code should take initial values, step size, and number of steps as inputs or define them as constants. To make it simple, maybe hardcode initial x and y, step size h, and the number of steps. Then, in the code comments, mention that these can be adjusted.

Also, need to be cautious about division by zero in the function (x + y) could be zero. The code should handle cases where x + y is zero, but since the user hasn't specified error handling, perhaps just note that in the comments.

Let me outline the code:

1. Include standard headers (stdio.h, math.h if needed, but here maybe not necessary). 2. Define the function f(x, y) which returns (x - y)/(x + y).
3. Implement the RK4 step function that takes current x, y, step h, computes k1-k4, updates y.
4. In main, set initial x0, y0, h, number of steps.
5. Loop over the steps, compute each step, print x and y.

But let's test this with an example. Suppose initial x0 is 1, y0 is 1, h=0.1, and compute a few steps. Wait, but if x0=1 and y0=1, then x + y = 2, so the function is (1-1)/2 = 0. So the first step would compute k1 to k4 based on that. However, if x and y start at 0, the denominator is zero. So need to ensure the initial values don't cause division by zero. So perhaps in the code example, choose x0=1, y0=2, just to have a safe starting point.

Alternatively, ask the user for input. But since the user didn't specify, it's better to hardcode values and mention in comments to adjust them.

Another point: in the code, when computing x, since the ODE is dy/dx = f(x,y), each step advances x by h. So in the main loop, after each step, x increases by h.

Putting it all together:

The code will have:

- Function definition for f(x, y)
- A function to perform one RK4 step
- Main function with loop

Wait, but the step function can be part of the loop. Alternatively, write a function that takes x, y, h, and returns the new y.

Yes, that's better. So:

```cpp
double rungeKuttaStep(double x, double y, double h) {
    compute k1, k2, k3, k4
    return y + (k1 + 2*k2 + 2*k3 + k4)/6;
}
```

But since each k depends on evaluations at different points, need to compute them step by step.

So here's the step-by-step code:

```cpp

#include <stdio.h>

double f(double x, double y) {
    return (x - y) / (x + y);
}

void rungeKutta4(double *x, double *y, double h) {
    double k1 = h * f(*x, *y);
    double k2 = h * f(*x + h/2, *y + k1/2);
    double k3 = h * f(*x + h/2, *y + k2/2);
    double k4 = h * f(*x + h, *y + k3);

    *y += (k1 + 2*k2 + 2*k3 + k4) / 6;
    *x += h;
}

int main() {
    double x0 = 1.0;  // Initial x
    double y0 = 2.0;  // Initial y
    double h = 0.1;    // Step size
    int steps = 10;    // Number of steps

    printf("Initial conditions: x = %f, y = %f\n", x0, y0);
    printf("Step size: %f\n", h);
    printf("Number of steps: %d\n\n", steps);

    for (int i = 0; i < steps; i++) {
        rungeKutta4(&x0, &y0, h);
        printf("Step %d: x = %f, y = %f\n", i+1, x0, y0);
    }

    return 0;
}
```

Wait, no: in each call to rungeKutta4, x is incremented by h. But the loop in main is called steps times, so after each step, x increases by h. That's correct.

Wait, in the initial code, x0 starts at 1.0. Then, after the first call to rungeKutta4, x0 becomes 1.0 + h (since *x +=h). So the first step's x is 1.1, and the y is updated. That's correct.

But in RK4, when you compute k1, you use x_n, then k2 and k3 use x_n + h/2, and k4 uses x_n + h. Then, after all, x increments by h. So that's accurate.

Now, let's check if the function f is correctly implemented. Yes, (x - y)/(x + y).

But wait, the user wrote the differential equation as (x - y)/(x + y). So dy/dx = (x - y)/(x + y). The code correctly returns that.

Potential issues: division by zero when x + y is zero. But the example initial conditions x0=1.0 and y0=2.0 avoid that. The user may need to adjust initial conditions accordingly.

Also, the code prints after each step, showing x and y.

But the user might want to integrate over a specific interval. Let's see: if h=0.1 and steps=10, x goes from 1.0 to 1.0 + 10*0.1=2.0.

Yes. So the code as written is correct.

I should test this code to ensure there are no syntax errors. Let me see:

- The function rungeKutta4 takes pointers to x and y, modifies them in place.
- The main function loops steps times, each time calling the function and printing.
- The f function is correctly defined.

Yes, this should compile and run.

However, in the main loop, the initial print is before the loop, so the initial x and y are printed, then the loop starts. But the first call to rungeKutta4 modifies x0 and y0 to the next step. Then, the first printed step is after the first update. So the first printed step is Step 1: x=1.1, y= updated value.

If the user wants to see the values at each step including the initial, maybe the code should print before updating. But in the code as written, the initial conditions are printed once, then the loop starts, updates, and prints the new x and y each time. So the output shows the progression from the initial step onwards. That's okay.

Alternatively, adjust the code to print after each step, which is what is done here.

I think this code should work. Therefore, the answer should include this code with explanations of the parameters and a note about potential division by zero.
</think>

Here's a C implementation of the 4th-order Runge-Kutta method for the differential equation **dy/dx = (x - y)/(x + y)**:

```cpp
#include <stdio.h>

// Define the differential equation dy/dx = (x - y)/(x + y)
double f(double x, double y) {
    return (x - y)/(x + y);
}

// Performs one iteration of RK4 method
void rk4_step(double *x, double *y, double h) {
    double k1 = h * f(*x, *y);
    double k2 = h * f(*x + h/2, *y + k1/2);
    double k3 = h * f(*x + h/2, *y + k2/2);
    double k4 = h * f(*x + h, *y + k3);

    *y += (k1 + 2*k2 + 2*k3 + k4)/6;
    *x += h;
}

int main() {
    // Initial conditions
    double x0 = 1.0;    // Initial x-value
    double y0 = 0.5;    // Initial y-value
    double h = 0.1;     // Step size
    int steps = 10;     // Number of steps

    printf("Starting at: x = %.2f, y = %.2f\n", x0, y0);
    printf("Step size: %.2f\n", h);
    printf("Number of steps: %d\n\n", steps);

    for(int i = 1; i <= steps; i++) {
        rk4_step(&x0, &y0, h);
        printf("Step %2d: x = %.3f, y = %.6f\n", i, x0, y0);
    }

    return 0;
}
```

**Key components:**
1. `f(x, y)` - Implements the differential equation
2. `rk4_step()` - Performs one RK4 iteration
3. `main()` - Controls the simulation with parameters:
   - Initial conditions (x0, y0)
   - Step size (h)
   - Number of steps

**Notes:**
- Adjust `x0`, `y0`, `h`, and `steps` as needed
- Be cautious when x + y = 0 (potential division by zero)
- Accuracy depends on step size h (smaller h = more precision)
- Output shows x/y values at each step

To use this program:
1. Compile with any C compiler (e.g., `gcc rk4.c -o rk4`)
2. Run the executable
3. Modify initial values for different starting conditions

Would you like me to explain any part in more detail or modify specific aspects of the implementation?

</spoiler>



–ù–∞—Å–∫–æ–ª—å–∫–æ —ç—Ç–æ—Ç –æ–ø—É—Å –ª—É—á—à–µ —Ç–æ–≥–æ, —á—Ç–æ –±—ã–ª –ø—Ä–∏–≤–µ–¥—ë–Ω –≤—ã—à–µ, —ç—Ç–æ —É–∂–µ –¥—Ä—É–≥–æ–π –≤–æ–ø—Ä–æ—Å, –Ω–æ —Ä–∞–∑–∏—Ç–µ–ª—å–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –æ—Ç–ª–∏—á–∞—é—â–∏–π—Å—è –æ—Ç–≤–µ—Ç –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –ø—Ä–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ–º –ø—Ä–æ–º–ø—Ç–µ ‚Äî —ç—Ç–æ –ª—é–±–æ–ø—ã—Ç–Ω—ã–π –º–æ–º–µ–Ω—Ç (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—É—Å–∫–∞ —è, —è—Å–Ω–æ–µ –¥–µ–ª–æ, –Ω–∏–∫–∞–∫ –Ω–µ –º–µ–Ω—è–ª).

–ö—Å—Ç–∞—Ç–∏, –æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö. –ï—Å—Ç—å –¥–≤–∞ —Ä–µ–∂–∏–º–∞ –∑–∞–ø—É—Å–∫–∞.

–ü–µ—Ä–≤—ã–π –≤–æ—Ç —Ç–∞–∫–æ–π, –æ–Ω –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è "text generation":

```
llama-cli.exe -m your_model.gguf -p "Hi!" -n 4096
```

–ê –≤—Ç–æ—Ä–æ–π —Å –¥—Ä—É–≥–∏–º –∫–ª—é—á–æ–º ‚Äî "–°hat (conversation)" 

```
llama-cli.exe -m your_model.gguf -p "Hello!" -cnv
```

–ü–µ—Ä–≤—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Ç–æ–º —Å–ª—É—á–∞–µ, –µ—Å–ª–∏ –∏—Å–∫—É—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –ø–∏—à–µ—Ç –∑–∞ –≤–∞—Å –¥–∏—Å—Å–µ—Ä—Ç–∞—Ü–∏—é, –∞ –≤—Ç–æ—Ä–æ–π ‚Äî –µ—Å–ª–∏ –æ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–∞ (—á–∞—Ç-–±–æ—Ç–∞). –ü—Ä–∏ —ç—Ç–æ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –≥–æ–≤–æ—Ä–∏—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å "–∑–∞—Ç–æ—á–µ–Ω–∞" –ø–æ–¥ —ç—Ç–æ—Ç —Ä–µ–∂–∏–º.

–Ø –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª –∏ —Ç–∞–∫ –∏ —Å—è–∫ –∏ –æ—Å–Ω–æ–≤–Ω–æ–µ –æ—Ç–ª–∏—á–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –∑–∞–º–µ—Ç–∏–ª, —ç—Ç–æ —Ç–æ, —á—Ç–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ "—á–∞—Ç –±–æ—Ç–∞" —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞—é—Ç—Å—è –≥–æ—Ç–æ–≤—ã–º–∏ —Ñ–æ—Ä–º—É–ª–∞–º–∏ –≤ TeX —Ñ–æ—Ä–º–∞—Ç–µ, –≤—ã–≥–ª—è–¥–∏—Ç —ç—Ç–æ –≤–æ—Ç —Ç–∞–∫, –µ—Å–ª–∏ —è –≤—Å—Ç–∞–≤–ª—é –≤—ã–≤–æ–¥ –ø—Ä—è–º–æ –≤ —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ö–∞–±—Ä–∞:

#### Explanation:
1. **Function `f`**: Implements the differential equation 

$$
( \frac{dy}{dx} = \frac{x - y}{x + y} ).
$$

2. **RK4 Method**:
   - Takes initial conditions `(x0, y0)`, step size `h`, and target `x_target`.
   - Iteratively computes the next y using the RK4 formulas:
     $$
     k_1 = h \cdot f(x_n, y_n),
     $$
     
     $$
     k_2 = h \cdot f(x_n + \frac{h}{2}, y_n + \frac{k_1}{2}),
     $$
     
     $$
     k_3 = h \cdot f(x_n + \frac{h}{2}, y_n + \frac{k_2}{2}),
     $$
     
     $$
     k_4 = h \cdot f(x_n + h, y_n + k_3),
     $$
     
     $$
     y_{n+1} = y_n + \frac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4).
     $$
     
   - Adjusts the step size near the target to avoid overshooting.

–í–æ–æ–±—â–µ —è –≤–µ—Ä—é, —á—Ç–æ –µ—Å–ª–∏ —è –ø–æ–ø—Ä–æ—à—É –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å —Ñ–æ—Ä–º—É–ª—ã –≤ —Ç–∞–∫–æ–º —Ñ–æ—Ä–º–∞—Ç–µ, –æ–Ω–∞ —ç—Ç–æ —Å–¥–µ–ª–∞–µ—Ç, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ä–µ–∂–∏–º–∞ –∑–∞–ø—É—Å–∫–∞, –Ω–æ –ø—Ä–æ—Å—Ç–æ –≤–æ—Ç —Ç–∞–∫–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ. –ù–∞–¥–æ –±—É–¥–µ—Ç –ø–æ—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏.

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

–û–Ω–∞, –∫–æ–Ω–µ—á–Ω–æ —É–¥—Ä—É—á–∞–µ—Ç. –ë–µ–∑—É—Å–ª–æ–≤–Ω–æ, –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–∞—è –ø–∞–º—è—Ç—å —Ç–∏–ø–∞ DDR5 —Å–ª–µ–≥–∫–∞ —É–ª—É—á—à–∏—Ç —Å–∏—Ç—É–∞—Ü–∏—é. –í –∏–¥–µ–∞–ª–µ –Ω–∞–¥–æ –∏–º–µ—Ç—å –±—ã—Å—Ç—Ä—É—é GPU –ø–∞–º—è—Ç—å, –Ω–æ –ø—è—Ç–æ–∫ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ —Ç–∏–ø–∞ NVIDIA H200, —É –∫–æ—Ç–æ—Ä—ã—Ö –ø–æ 141 –ì–ë –ø–∞–º—è—Ç–∏ —Å –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é –ø–æ–¥ –ø—è—Ç—å —Ç–µ—Ä–∞–±–∞–π—Ç –≤ —Å–µ–∫—É–Ω–¥—É —Å—Ä–∞–∑—É –æ—Ç–ø—Ä–∞–≤—è—Ç –Ω–∞—Å –≤ –º–∏—Ä —à–µ—Å—Ç–∏–∑–Ω–∞—á–Ω—ã—Ö —Ü–µ–Ω–Ω–∏–∫–æ–≤.

–ß—Ç–æ –ø–æ —ç—Ç–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, —Ç–æ —Ç–∞–º –º–æ–∂–Ω–æ –µ—â—ë –Ω–µ–º–Ω–æ–≥–æ –ø–æ–ø—ã—Ç–∞—Ç—å—Å—è –ø–æ–¥–∫—Ä—É—Ç–∏—Ç—å –≤ BIOS

–ù–µ–∫–æ—Ç–æ—Ä—ãq –ø–æ—Ç–µ–Ω—Ü–∞–ª, –≤–æ–∑–º–æ–∂–Ω–æ, –∫—Ä–æ–µ—Ç—Å—è –≤ NUMA:

![image-20250206104206792](assets/image-20250206104206792.png)

–ü–æ–∫–∞ —á—Ç–æ —Ç–∞–º –Ω–µ –≤—Å—ë —Ö–æ—Ä–æ—à–æ, –Ω–æ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å —ç—Ç–∏–º —É–∂–µ –æ–∑–∞–±–æ—Ç–∏–ª–∞—Å—å, –Ω–∞ –≥–∏—Ç—Ö–∞–±–µ –≤–∏—Å–∏—Ç –æ—Ç–∫—Ä—ã—Ç—ã–π —Ç–∏–∫–µ—Ç - [Feature Request: NUMA-aware MoE Expert Allocation for Improved Performanc #11333](https://github.com/ggerganov/llama.cpp/issues/11333). –í–æ–æ–±—â–µ –∫–æ–≥–¥–∞ –º—ã —É–ø–∏—Ä–∞–µ–º—Å—è –≤ –ø—Ä–æ–ø—É—Å–∫–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø–∞–º—è—Ç–∏ ‚Äî –∫–∞–∫ –æ—Ç–º–µ—Ç–∏–ª–∏ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ö –∫ –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ–º—É –ø–æ—Å—Ç—É —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –Ω–µ –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ –∏–º–µ—Ç—å –¥–≤–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞, –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—â–∏—Ö –∑–∞ –æ–±—â—É—é –ø–∞–º—è—Ç—å (—Ö–æ—Ç—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –¥–≤–µ–Ω–∞–¥—Ü–∞—Ç–∏–∫–∞–Ω–∞–ª—å–Ω–æ–π –ø–∞–º—è—Ç—å—é –Ω–∞ –¥–≤—É—Ö–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ ‚Äî —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω–∞—è —Ç–µ–º–∞, —Ç—Ä–µ–±—É—é—â–∞—è –¥–æ—Ç–æ—à–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤).

–ü–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –∑–∞–ø—É—Å–∫–∞ —è —Å–∏–ª—å–Ω–æ –Ω–µ –ª–∞–∑–∞–ª, –≤–æ–∑–º–æ–∂–Ω–æ —Ç–∞–º —Ç–æ–∂–µ —á—Ç–æ-—Ç–æ –º–æ–∂–Ω–æ –ø–æ–¥–∫—Ä—É—Ç–∏—Ç—å.

–ú–æ–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–±—Ä–∞—Ç—å –≤—Å—ë –∏–∑ –∏—Å—Ö–æ–¥–Ω–∏–∫–æ–≤ –∏ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, –≥–¥–µ –ø—Ä–æ—Å–µ–¥–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–π—Ç–∏—Å—å –ø–æ "–≥–æ—Ä—è—á–∏–º —Ç–æ—á–∫–∞–º", –¥–∞ —Ö–æ—Ç—å —Ç–µ–º –∂–µ Intel VTune.

–ù–æ —É–∂–µ –ø–æ–Ω—è—Ç–Ω–æ, —á—Ç–æ —Å–∏–ª—å–Ω–æ –±—ã—Å—Ç—Ä–µ–µ –æ—Ç —ç—Ç–æ–≥–æ –Ω–µ —Å—Ç–∞–Ω–µ—Ç, –∏ –¥–∞–∂–µ –µ—Å–ª–∏ —Ä–∞—Å–∫–æ—á–µ–≥–∞—Ä–∏—Ç—å –≤—Å—ë –≤–¥–≤–æ–µ-–≤—Ç—Ä–æ–µ, —ç—Ç–æ–≥–æ –±—É–¥–µ—Ç –º–∞–ª–æ, –Ω–∞–¥–æ —Ä–∞–∑ —ç—Ç–∞–∫ –≤ –¥–≤–∞–¥—Ü–∞—Ç—å –±—ã—Å—Ç—Ä–µ–µ.

–ï—Å–ª–∏ –±—Ä–∞—Ç—å –Ω–µ —Ç–∞–∫–∏–µ —É–≤–µ—Å–∏—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏, —Ç–æ, –∫–æ–Ω–µ—á–Ω–æ, –≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç —Ä–µ–∑–≤–µ–µ, –Ω–æ –∏ –∫–∞—á–µ—Å—Ç–≤–æ –≤—ã–≤–æ–¥–∞ –ø–∞–¥–∞–µ—Ç ‚Äî –¥–ª—è —Ç–æ–≥–æ –∂–µ –†—É–Ω–≥–µ-–ö—É—Ç—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –µ—â—ë –≤—ã–¥–∞—ë—Ç—Å—è, –∞ –≤–æ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ - —É–∂–µ –Ω–µ—Ç (—Ö–æ—Ç—è –µ—Å—Ç—å "coder" –º–æ–¥–µ–ª–∏, –∑–∞—Ç–æ—á–µ–Ω–Ω—ã–µ –∫–∞–∫ —Ä–∞–∑ –ø–æ–¥ –Ω–∞–ø–∏—Å–∞–Ω–∏–µ –∫–æ–¥–∞, –Ω–∞–¥–æ –±—É–¥–µ—Ç –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å).