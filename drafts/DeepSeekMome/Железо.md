–ü—Ä–∏–≤–µ—Ä, –•–∞–±—Ä!

–í –∫–æ–ø–∏–ª–∫—É –ø–æ—Å—Ç–æ–≤ –ø—Ä–æ DeepSeek, –æ –∫–æ—Ç–æ—Ä–æ–º –Ω–µ –ø–∏—Å–∞–ª —Ä–∞–∑–≤–µ —á—Ç–æ —Å–æ–≤—Å–µ–º –ª–µ–Ω–∏–≤—ã–π —Ö–æ—á—É –∑–∞–±—Ä–æ—Å–∏—Ç—å –µ—â—ë –ø—è—Ç—å –∫–æ–ø–µ–µ–∫ –≤ –≤–∏–¥–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç—á—ë—Ç–∞ –æ –∏–Ω—Å—Ç–∞–ª–ª—è—Ü–∏–∏ –Ω–∞ Xeon, –æ –∫–æ—Ç–æ—Ä–æ–º –º–µ–Ω—è –ø–æ–ø—Ä–æ—Å–∏–ª–∏ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö. –ö–æ–º—É –ª—é–±–æ–ø—ã—Ç–Ω–æ - –º–æ–∂–µ—Ç –∑–∞–≥–ª—è–Ω—É—Ç—å –ø–æ–¥ –∫–∞—Ç, –Ω—É –∞ —Ç–µ–º, –∫—Ç–æ —É–∂–µ –≤—ã–ø–æ–ª–Ω–∏–ª —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ –ø–æ —É—Å—Ç–∞–Ω–æ–≤–∫–µ - –±—É–¥–µ—Ç —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –Ω–µ–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ.

–ü—Ä–∏–∫–æ—Å–Ω—É—Ç—å—Å—è –∫ –ò–ò.

–°–ø–æ–π–ª–µ—Ä –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ —á–∏—Ç–∞—é—â–∏—Ö - —è –ø—Ä–æ—Å—Ç–æ —Å–∫–∞—á–∞—é DeepSeek –∏ –∑–∞–ø—É—â—É –µ–≥–æ —á–µ—Ä–µ–∑ llama.cpp, –Ω–∞ –∫–∞–∫—É—é-–ª–∏–±–æ –Ω–∞—É—á–Ω—É—é –Ω–æ–≤–∏–∑–Ω—É —ç—Ç–æ—Ç –ø–æ—Å—Ç —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –Ω–µ –ø—Ä–µ—Ç–µ–Ω–¥—É–µ—Ç. –ó–∞—á–µ–º —ç—Ç–æ –Ω–∞ —Ö–∞–±—Ä–µ? –ü—Ä–æ—Å—Ç–æ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö –∫ –ø–æ—Å—Ç—É "" —è —Å–ø—Ä–æ—Å–∏–ª "–∏–º–µ–µ—Ç–µ –ª–∏ —Å–º—ã—Å–ª –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–∞ —Ç–æ–º –∂–µ–ª–µ–∑–µ, —á—Ç–æ —É –º–µ–Ω—è –µ—Å—Ç—å", –∏ –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ "–ø–ª—é—Å–∏–∫–æ–≤" –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –∫–æ–º—É-–Ω–∏–±—É–¥—å —ç—Ç–æ –±—É–¥–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ, –Ω—É –∞ —Ä–∞–∑ –ø–∞–∑–∑–ª —Å–ª–æ–∂–∏–ª—Å—è - –æ —Ç–æ–º –∏ –ø–æ—Å—Ç, –ø—Ä–æ—Å—Ç–æ —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç–æ–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É. –ò –¥–∞, —è –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –¥–∏–ª–µ—Ç–∞–Ω—Ç –≤ LLM, —ç—Ç–æ –ø–µ—Ä–≤–∞—è "–ø—Ä–∏—Å—Ç—Ä–µ–ª–∫–∞", —Ç–∞–∫ —á—Ç–æ "—Ç—É—Ç–æ—Ä–∏–∞–ª–æ–º" –æ—Ñ–æ—Ä–º–ª—è—Ç—å –ø–æ—Å—Ç –Ω–µ –±—É–¥—É.

–í—Ç–æ—Ä–æ–π —Å–ø–æ–π–ª–µ—Ä - –¥–∞ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç. –ù–æ –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ. –ù–æ —Ä–∞–±–æ—Ç–∞–µ—Ç.

### –ñ–µ–ª–µ–∑–æ

–£–ø—Ä–∞–∂–Ω—è—Ç—å—Å—è —è –±—É–¥—É –≤–æ—Ç –Ω–∞ —Ç–∞–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:

HP z8 G4 —Ä–∞–±–æ—á–∞—è —Å—Ç–∞–Ω—Ü–∏—è –ø—Ä–∏–º–µ—Ä–Ω–æ –ø—è—Ç–∏–ª–µ—Ç–Ω–µ–π –¥–∞–≤–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Å—Ç–æ–∫–æ–≤–æ–º –≤–∞—Ä–∏–∞–Ω—Ç–µ.

–î–≤–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ Xeon Gold 6132, 768 GB –ø–∞–º—è—Ç–∏ DDR4, —Ç–µ—Ä–∞–±–∞–π—Ç–Ω—ã–π NVMe SSD Samsung. 

CPU-z –≤—ã–¥–∞—ë—Ç –º–Ω–µ 10000+ –ø–æ–ø—É–≥–∞–µ–≤:

![image-20250205090637918](assets/image-20250205090637918.png)

–ü–∞–º—è—Ç—å –Ω–∞–±—Ä–∞–Ω–∞ –ø–ª–∞—à–∫–∞–º–∏ –ø–æ 64 –ì–ë, –∏—Ö —Ç–∞–º –¥–≤–µ–Ω–∞–¥—Ü–∞—Ç—å —à—Ç—É–∫:

![image-20250205090742608](assets/image-20250205090742608.png)

–ë–µ–Ω—á–º–∞—Ä–∫ –ø–∞–º—è—Ç–∏ –∏ –∫–µ—à–∞ —è —É–∂–µ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ö –ø–æ–∫–∞–∑—ã–≤–∞–ª, –≤—ã–Ω–µ—Å—É —Å—é–¥–∞ —Ç–æ–∂–µ:

![image-20250205092255850](assets/image-20250205092255850.png)

–í–∏–¥–µ–æ–∫–∞—Ä—Ç —Ç–∞–º –¥–≤–µ - NVidia Quadro K4200 –¥–∞ AMD Radeon Pro WX 9100, –Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∫–∞–∫ GPU –æ–Ω–∏ –Ω–µ –±—É–¥—É—Ç (—Å–º—ã—Å–ª–∞ –≤ –æ–±—â–µ–º –Ω–µ—Ç).

![image-20250205090845219](assets/image-20250205090845219.png)

–î–∏—Å–∫:

![image-20250205091725447](assets/image-20250205091725447.png)

–í–æ–æ–±—â–µ –∏—Ö —Ç–∞–º —á–µ—Ç—ã—Ä–µ —Ç–∞–∫–∏—Ö, –Ω–æ –≤ —Ä–µ–π–¥ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –Ω–µ –±—É–¥—É, –º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑ –≥—Ä—É–∑–∏—Ç—Å—è –≤ –ø–∞–º—è—Ç—å, –≤ –ø—Ä–∏–Ω—Ü–∏–ø–µ –∏ —Ç–∞–∫ –Ω–æ—Ä–º.

–ö–æ–º–ø –æ—Å—Ç–∞–ª—Å—è –∫–∞–∫ —Ç–µ—Å—Ç–æ–≤—ã–π –æ—Ç –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞,  –≤ –∫–æ—Ç–æ—Ä–æ–º –Ω–∞–¥–æ –±—ã–ª–æ –±—ã—Å—Ç—Ä–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–æ–ª—Ç–µ—Ä–∞–±–∞–π—Ç–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫, –ø—Ä–∏–ª–µ—Ç–∞—é—â–∏—Ö –æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–∫–æ—Ä–æ—Å—Ç–Ω—ã—Ö –∫–∞–º–µ—Ä, —Ç–µ–ø–µ—Ä—å –ø—Ä–æ—Å—Ç–æ –ø—ã–ª–∏—Ç—Å—è –ø–æ–¥ —Å—Ç–æ–ª–æ–º –∏ —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π.

### –°–∫–∞—á–∏–≤–∞–µ–º DeepSeek R0

–Ø –±—ã –º–æ–≥ –ø—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—Å–∞—Ç—å "—Å–∫–∞—á–∞–π—Ç–µ DeepSeek", –Ω–æ —Ç—É—Ç –µ—Å—Ç—å –Ω–µ–±–æ–ª—å—à–æ–π –Ω—é–∞–Ω—Å. –î–µ–ª–æ –≤ —Ç–æ–º, —á—Ç–æ –Ω–∞ —Ä–∞–±–æ—Ç–µ —É –º–µ–Ω—è –≤—Å–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—ã —Å—Ç–∞—Ä–∞—Ç–µ–ª—å–Ω–æ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã –∑–ª—ã–º –∞–¥–º–∏–Ω–æ–º, –æ—Ö—Ä–∞–Ω—è—é—â–∏–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å. –£ –º–µ–Ω—è –Ω–µ—Ç –æ–Ω–ª–∞–π–Ω –¥–æ—Å—Ç—É–ø–∞ –Ω–∏ –∫ ChatGPT, –Ω–∏ –∫ Perplexity, –Ω–∏ –∫ DeepSeek, —Ä–∞–≤–Ω–æ –∫–∞–∫ –≤—Å–µ –¥—Ä–æ–ø–±–æ–∫—Å—ã, –≥—É–≥–ª–æ–¥—Ä–∞–π–≤—ã, –æ–Ω–ª–∞–π–Ω –∑–∞–º–µ—Ç–æ—á–Ω–∏–∫–∏ - –±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ —Ä–µ—à–∏—Ç–µ–ª—å–Ω–æ –≤—Å—ë. –ü—Ä–∏—á—ë–º –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–æ–∫—Å–∏, –Ω–æ —Ç–∞–∫–∂–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è MITM –ø–æ–¥–º–µ–Ω–æ–π —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤, –ø–æ—ç—Ç–æ–º—É —Å–∫–∞—á–∞—Ç—å —Å huggingface —è –Ω–∏—á–µ–≥–æ –Ω–µ –º–æ–≥—É, –ø–æ–ª—É—á–∞—è –æ—Ç–ª—É–ø 500. –ò–∑–¥–µ—Ä–∂–∫–∏ —Ä–∞–±–æ—Ç—ã –≤ –±–æ–ª—å—à–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –¢–∞–∫ —á—Ç–æ –∫–∞—á–∞—Ç—å —è –±—É–¥—É –¥–æ–º–∞. –ê –¥–æ–º–∞ —É –º–µ–Ω—è –≤—Å–µ–≥–æ-–Ω–∞–≤—Å–µ–≥–æ 100 –ú–±–∏—Ç (—Ö–æ—Ç—å –∏ –æ–ø—Ç–æ–≤–æ–ª–æ–∫–Ω–æ), —Ç–∞–∫ —á—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –Ω–µ –±—ã—Å—Ç—Ä—ã–π (–ø–ª–∞–Ω–∏—Ä—É–π—Ç–µ –±–æ–ª—å—à–µ —Å—É—Ç–æ–∫).

–ü–æ—Å–∫–æ–ª—å–∫—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π, —Ç–æ —è —Ä–µ—à–∏–ª, —á—Ç–æ —è —Å–∞–º—ã–π —É–º–Ω—ã–π, –∏ —Å–ª–µ–≥–∫–∞ –ø–æ–≥—É–≥–ª–∏–≤ –Ω–∞—à—ë–ª —Å–ø–æ—Å–æ–± –≤—ã–¥–µ—Ä–Ω—É—Ç—å —á–µ—Ä–µ–∑ –≥–∏—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É, –∞ –Ω–µ –≤—Å—é —Ä–µ–ø—É.

–í–æ—Ç —Ç–∞–∫ —ç—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è:

```
git ...
```

–Ø –≤–∫–ª—é—á–∏–ª —Å—Ç–∞—Ä—ã–π –∫–æ–º–ø —Å —Ñ–∞–π–ª–æ–ø–æ–º–æ–π–∫–æ–π , –ø—Ä–æ–≤–µ—Ä–∏–ª —Ç–∞–º —Å–≤–æ–±–æ–¥–Ω–æ–µ –º–µ—Å—Ç–æ, –≤—ã–ø–æ–ª–Ω–∏–ª –∫–æ–º–∞–Ω–¥—ã –≤—ã—à–µ –∏ –ø–æ—à—ë–ª —Å–ø–∞—Ç—å. –ù–∞—É—Ç—Ä–æ —è –æ–±–Ω–∞—Ä—É–∂–∏–ª, —á—Ç–æ —Ç—É–¥–∞ –ø—Ä–∏–ª–µ—Ç–µ–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏ –æ–Ω –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏–ª—Å—è. –ù—É, –±—ã–≤–∞–µ—Ç.

–õ–∏—Ä–∏—á–µ—Å–∫–æ–µ –æ—Ç—Å—Ç—É–ø–ª–µ–Ω–∏–µ - –µ—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–∞–∑–∞—Ç—å—Å—è –æ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π, —Ç–æ –ø—Ä–æ—Å—Ç–æ –≤–∫–ª—é—á–∏—Ç–µ Metered Network –¥–ª—è —Å–µ—Ç–µ–≤–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞, —ç—Ç–æ —Å–∞–º—ã–π –Ω–∞–∏–ø—Ä–æ—Å—Ç–µ–π—à–∏–π —Å–ø–æ—Å–æ–±.

–Ø —ç—Ç–æ —Å–¥–µ–ª–∞–ª, –∏ –≤—ã–ø–æ–ª–Ω–∏–≤ –∫–æ–º–∞–Ω–¥—ã –≤—Ç–æ—Ä–æ–π —Ä–∞–∑, –æ–±–Ω–∞—Ä—É–∂–∏–ª, —á—Ç–æ "–¥–æ–∫–∞—á–∫–∏" —Ç–∞–º –Ω–µ—Ç –∏ –≤ –ø–æ–º–∏–Ω–µ, git –Ω–∞—á–∞–ª –∫–∞—á–∞—Ç—å —Å –Ω—É–ª—è (—ç—Ç–æ –≤–∏–¥–Ω–æ –≤ lfs –∫–µ—à–µ, —Ç–∞–º —Å–Ω–∞—á–∞–ª–∞ —Ñ–∞–π–ª—ã —Å–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É). –¢—É—Ç —è –≤—Å–ø–æ–º–Ω–∏–ª, —á—Ç–æ —É –º–µ–Ω—è –≤–∞–ª—è–µ—Ç—Å—è –¥—Ä–µ–≤–Ω–∏–π QNAP NAS, —Ç–∞–º –µ—Å—Ç—å –∫–∞—á–∞–ª–∫–∞, –Ω–æ —Ç—É—Ç –º–µ–Ω—è –∂–¥–∞–ª–∞ –¥—Ä—É–≥–∞—è –∑–∞—Å–∞–¥–∞ - —Ä–∞–∑ –≤ —Å—É—Ç–∫–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Å–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ, –ø—Ä–∏ —ç—Ç–æ–º NAS –≤—ã–≤–∞–ª–∏–≤–∞–ª –æ—à–∏–±–∫—É –∏ –Ω–∞—á–∏–Ω–∞–ª —Å–∫–∞—á–∫—É —Å–Ω–æ–≤–∞. –° —Ç–æ—Ä—Ä–µ–Ω—Ç–∞–º–∏ –æ–Ω —Ö—É–¥–æ-–±–µ–¥–Ω–æ —Å–ø—Ä–∞–≤–ª—è–ª—Å—è, –∞ –≤–æ—Ç —Å https –Ω–µ—Ç. –Ø —É–∂–µ —Ö–æ—Ç–µ–ª –±—ã–ª–æ –æ—Ç–∫–∞–∑–∞—Ç—å—Å—è –æ—Ç –∑–∞—Ç–µ–∏, –Ω–æ –≤—ã –∂–µ –ø–æ–Ω–∏–º–∞–µ—Ç–µ ‚Äî –µ—Å–ª–∏ —è –≤–æ –≤—Å–µ—É—Å–ª—ã—à–∞–Ω–∏–µ –Ω–∞–ø–∏—à—É –∑–¥–µ—Å—å, —á—Ç–æ –Ω–µ —Å–º–æ–≥ —Å–∫–∞—á–∞—Ç—å —à–µ—Å—Ç—å—Å–æ—Ç –≥–∏–≥, —Ç–æ –Ω–∞–¥–æ –º–Ω–æ–π –±—É–¥–µ—Ç —Ä–∂–∞—Ç—å –≤–µ—Å—å —Ö–∞–±—Ä, –∏ —è –±—É–¥—É –ø–æ–¥–≤–µ—Ä–≥–Ω—É—Ç –ø—É–±–ª–∏—á–Ω–æ–º—É –æ—Å—Ç—Ä–∞–∫–∏–∑–º—É, —Ç–∞–∫ —á—Ç–æ –ø—Ä–∏—à–ª–æ—Å—å —Ä–∞—Å—á–µ—Ö–ª–∏—Ç—å  –º–µ–Ω–µ–¥–∂–µ—Ä –∑–∞–∫–∞—á–µ–∫, –∫–æ—Ç–æ—Ä—ã–º —è –Ω–µ –ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è —É–∂–µ –º–Ω–æ–≥–æ –ª–µ—Ç.

–Ø —ç—Ç–æ –≤—Å—ë –∫ —Ç–æ–º—É, —á—Ç–æ –µ—Å–ª–∏ –∑–∞—Ö–æ—Ç–∏—Ç–µ —Å–∫–∞—á–∞—Ç—å –Ω–∞ —Å–ª–∞–±–æ–º –∫–∞–Ω–∞–ª–µ ‚Äî –∑–∞—Ä–∞–Ω–µ–µ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –≤—ã –º–æ–∂–µ—Ç–µ —É–≤–µ—Ä–µ–Ω–Ω–æ –¥–æ–∫–∞—á–∏–≤–∞—Ç—å –ø—Ä–∏ –æ–±—Ä—ã–≤–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è.

–ö–∞–∫ –±—ã —Ç–æ –Ω–∏ –±—ã–ª–æ, –≤–æ—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π, –≤–æ—Ç –º–æ–¥–µ–ª—å, –∞ –ø–æ–¥ —Å–ø–æ–π–ª–µ—Ä–æ–º - –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ —Å–∫–æ—Ä–º–∏—Ç—å –ª—é–±–æ–º—É –º–µ–Ω–µ–¥–∂–µ—Ä—É, —è –ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è [JDownloader 2](https://jdownloader.org/download/index). 

```
—Å–ø–∏—Å–æ–∫
```

–ü–ª—é—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –µ—â—ë –∏ –≤ —Ç–æ–º, —á—Ç–æ –¥–Ω—ë–º —è –º–æ–≥—É —É–º–µ–Ω—å—à–∏—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å, —á—Ç–æ–±—ã –Ω–µ –¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –Ω–µ—É–¥–æ–±—Å—Ç–≤ –¥–æ–º–∞—à–Ω–∏–º, –∞ –Ω–æ—á—å—é –æ—Ç–∫—Ä—ã–≤–∞—Ç—å –∫—Ä–∞–Ω –Ω–∞ –≤—Å—é –∫–∞—Ç—É—à–∫—É.

–ü–æ–∫–∞ –∏–¥—ë—Ç —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ, —á—Ç–æ–± –±—ã–ª–æ –Ω–µ —Å–∫—É—á–Ω–æ ‚Äî –º–æ–≥—É –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∑–∞–≥–∞–¥–∫—É.

–í–æ –≤—Ç–æ—Ä–æ–π –ø–æ–ª–æ–≤–∏–Ω–µ –¥–µ–≤—è–Ω–æ—Å—Ç—ã—Ö, –º—ã, –Ω–æ–≤–æ–∏—Å–ø–µ—á—ë–Ω–Ω—ã–µ –≤—ã–ø—É—Å–∫–Ω–∏–∫–∏ - —Ñ–∏–∑–∏–∫–∏, –ø–æ–¥—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–∞–º–∏, –∏–Ω–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞–ª–∏—Å—å –≤–º–µ—Å—Ç–µ, —Ä–∞—Å–ø–∏–≤–∞–ª–∏ –∫—Ä–µ–ø–∫–∏–µ —Å–ø–∏—Ä—Ç–Ω—ã–µ –Ω–∞–ø–∏—Ç–∫–∏ –∏ –∏–Ω–æ–≥–¥–∞ —Å–º–æ—Ç—Ä–µ–ª–∏ —Ñ–∏–ª—å–º—ã –Ω–∞ –≤–∏–¥–µ–æ–∫–∞—Å—Å–µ—Ç–∞—Ö. –í —á–∏—Å–ª–µ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –±—ã–ª —Ñ–∏–ª—å–º, –∫–æ—Ç–æ—Ä—ã–π –º—ã –ª–∞—Å–∫–æ–≤–æ –Ω–∞–∑—ã–≤–∞–ª–∏ –Ω–µ –∏–Ω–∞—á–µ –∫–∞–∫ "—ç—Ç–æ –Ω–∞—à–µ –∫–∏–Ω–æ, –ø—Ä–æ —ç—Ç–∏, –ø—Ä–æ –ì–ò–ì–ê–ë–ê–ô–¢–´!" –£–≥–∞–¥–∞–π—Ç–µ, –æ –∫–∞–∫–æ–º —Ñ–∏–ª—å–º–µ, –≤—ã–ø—É—â–µ–Ω–Ω–æ–º –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –¥–µ–≤—è–Ω–æ—Å—Ç—ã—Ö, —à–ª–∞ —Ä–µ—á—å? –ü–æ—Ç–æ–º —Å–ø—Ä–æ—Å–∏–º —É DeepSeek.

–ö–∞–∫ –±—ã —Ç–æ –Ω–∏ –±—ã–ª–æ, —á–µ—Ä–µ–∑ –ø–∞—Ä—É –¥–Ω–µ–π –∏ –Ω–æ—á–µ–π –Ω–∞ —Ç–µ—Ä–∞–±–∞–π—Ç–Ω–æ–º –¥–∏—Å–∫–µ —É –º–µ–Ω—è –ª–µ–∂–∞–ª–∏ –ø—è—Ç–Ω–∞–¥—Ü–∞—Ç—å –∑–∞–≤–µ—Ç–Ω—ã—Ö GGUF —Ñ–∞–π–ª–æ–≤. –ù–µ—Å—è –∏—Ö –Ω–∞ —Ä–∞–±–æ—Ç—É, –∏—Å–ø—ã—Ç–∞–ª —Å—Ç—Ä–∞–Ω–Ω–æ–µ –æ—â—É—â–µ–Ω–∏–µ ‚Äî —ç—Ç–æ —á—Ç–æ –∂–µ, —É –º–µ–Ω—è –≤ —Ä—É–∫–∞—Ö –ø–æ—á—Ç–∏ –≤–µ—Å—å –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π –∑–∞–ø–∞—Å –∑–Ω–∞–Ω–∏–π —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–∞?!

### –ó–∞–ø—É—Å–∫–∞–µ–º.

–í–æ–æ–±—â–µ –≥–æ–≤–æ—Ä—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–æ—Å–æ–±–æ–≤ "–∑–∞–ø—É—Å—Ç–∏—Ç—å" DeepSeek - LM Studio, Ollama, llama.ccp –∏ OpenVINO, —á–∞—Å—Ç—å –∏–∑ –Ω–∏—Ö —è –ø–æ—á–µ—Ä–ø–Ω—É–ª –∏–∑ –∫–æ–º–º–µ–Ω—Ç–æ–≤. –Ø –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª –Ω–∞–≤—Å–∫–∏–¥–∫—É –≤—Å–µ, –Ω–æ –±–µ–∑ —Ñ–∞–Ω–∞—Ç–∏–∑–º–∞.

–°–∞–º—ã–π –Ω–∞–∏–ø—Ä–æ—Å—Ç–µ–π—à–∏–π ‚Äî LM Studio. –ê–∫—Ç—É–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è 3.9.6. –Ø –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª –ø–æ—Å—Ç–∞–≤–∏—Ç—å –¥–æ–º–∞, –≤—Å—ë –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –∫–∞–∫ –¥–µ–º–∫–∞ —Å–∫–∞—á–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–µ–Ω—å–∫–∞—è DeepSeek R1 Distilled (Qwen 7B) –Ω–∞ 4 —Å –ø–æ–ª–æ–≤–∏–Ω–æ–π –≥–∏–≥–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–ø—É—Å—Ç–∏—Ç—å —ç—Ç–æ –¥–µ–ª–æ –¥–∞–∂–µ –Ω–∞ –º–æ—ë–º –¥—Ä–µ–≤–Ω–µ–º –Ω–æ—É—Ç–µ (32 –ì–ë, –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä i7-4940MX). –Ø –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª ‚Äî –¥–∞–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ. –§–∞–π–ª—ã –º–æ–¥–µ–ª–µ–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ª–µ–∂–∞—Ç –≤ %USERPROFILE%\\.lmstudio\models\lmstudio-community\, —Ç–∞–º –æ–Ω–∏ –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö —Ç–∏–ø–∞ \\DeepSeek-R1-Distill-Qwen-7B-GGUF.

–Ø –∑–∞–∫–∏–Ω—É–ª –Ω–∞ –¥–∏—Å–∫ –∏–Ω—Å—Ç–∞–ª–ª—è—Ç–æ—Ä, –Ω–æ –≤–æ—Ç –Ω–∞ hp z8 –º–µ–Ω—è –∂–¥–∞–ª –æ–±–ª–æ–º - "—Ç–µ—Å—Ç–æ–≤–∞—è" Qwen 7B —Ö–æ—Ç—å –∏ –∑–∞–≥—Ä—É–∂–∞–ª–∞—Å—å, –Ω–æ –≤—ã–≤–∞–ª–∏–≤–∞–ª–∞—Å—å –≤ "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—É—é –æ—à–∏–±–∫—É" –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –∂–µ –ø—Ä–æ–º–ø—Ç–∞, –∞ —Å —Ç–∞–∫–∏–º —Ç—Ä—É–¥–æ–º –≤—ã–∫–∞—á–∞–Ω–Ω–∞—è Q0 –æ—Ç–∫–∞–∑–∞–ª–∞—Å—å –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è –≤–æ–≤—Å–µ, —Å–æ—Å–ª–∞–≤—à–∏—Å—å –Ω–∞ –Ω–µ—Ö–≤–∞—Ç–∫—É —Ä–µ—Å—É—Ä—Å–æ–≤. –î–∞–ª—å—à–µ —è –ø—Ä–æ–±–æ–≤–∞—Ç—å –ø–æ–∫–∞ –Ω–µ —Å—Ç–∞–ª –∏ –æ—Ç–ª–æ–∂–∏–ª –∫—Ä–∞—Å–∏–≤—É—é –∏–≥—Ä—É—à–∫—É –≤ —Å—Ç–æ—Ä–æ–Ω—É.

Ollama –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞ –Ω–∞ —Ç–æ, —á—Ç–æ —Å–∫–∞—á–∏–≤–∞—Ç—å –º–æ–¥–µ–ª—å –≤—ã –±—É–¥–µ—Ç–µ —á–µ—Ä–µ–∑ –Ω–µ—ë –∂–µ (—á—Ç–æ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤ –º–æ—ë–º —Å–ª—É—á–∞–µ, –∞ –ø–µ—Ä–µ–∫–∞—á–∏–≤–∞—Ç—å –¥–æ–º–∞ –≤—Å—ë –µ—â—ë —Ä–∞–∑ ‚Äî —Ç—É—Ç —É–∂ –∏–∑–≤–∏–Ω–∏—Ç–µ), 

```
Welcome to Ollama!

Run your first model:

        ollama run llama3.2

PS C:\Windows\System32> ollama run deepseek-r1:1.5b
pulling manifest
pulling aabd4debf0c8... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.1 GB
pulling 369ca498f347... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  387 B
pulling 6e4c38e1172f... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.1 KB
pulling f4d24e9138dd... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  148 B
pulling a85fe2a2e58e... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  487 B
verifying sha256 digest
writing manifest
success
>>> hi!
<think>

</think>

Hello! How can I assist you today? üòä
```

—Ç–∞–∫ —Ç–æ –æ–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∞ –≤–æ—Ç –∫–∞–∫ "–ø–æ–¥–æ—Ç–∫–Ω—É—Ç—å" –µ–π —É–∂–µ —Å–∫–∞—á–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã GGUF ‚Äî —Ç—É—Ç —á—É—Ç—å —Å–ª–æ–∂–Ω–µ–µ. –¢–æ –µ—Å—Ç—å –º–µ—Å—Ç–æ-—Ç–æ –≥–¥–µ –æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –ª–µ–∂–∞—Ç—å —è –∑–Ω–∞—é %USERPROFILE%\\.ollama\models, –Ω–æ —Ç–∞–º –º–æ–¥–µ–ª—å –Ω–∞–¥–æ –∫–∏–¥–∞—Ç—å –≤ –ø–∞–ø–∫—É \blobs, –ø—Ä–∏ —ç—Ç–æ–º –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å —Ñ–∞–π–ª –≤ sha256-<—Ö–µ—à>, –∫—Ä–æ–º–µ —Ç–æ–≥–æ,  –Ω–∞–¥–æ –¥–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ \manifests\registry\\.ollama.ai\library\deepseek-r1, –≤ –æ–±—â–µ–º —Ç—É—Ç –Ω–∞–¥–æ "–ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å" –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ 

\> ollama create <your-model-name-here> -f Modelfile, –Ω–æ –ø–µ—Ä–µ–¥ —ç—Ç–∏–º –≤–∞–º –ø—Ä–∏–¥—ë—Ç—Å—è —Å–º–µ—Ä–∂–∏—Ç—å –≤—Å–µ –ø—è—Ç–Ω–∞–¥—Ü–∞—Ç—å —Ñ–∞–π–ª–æ–≤ –≤–º–µ—Å—Ç–µ —á–µ—Ä–µ–∑ gguf-split --merge infile-00001-of-0000N.gguf outfile.gguf, –∏ –≤ –æ–±—â–µ–º –Ω—É –µ–≥–æ –≤ —Ç–æ–ø–∫—É —Ç–∞–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è.

–ê –≤–æ—Ç —Å llama.ccp –≤—Å—ë –ø–æ–ª—É—á–∏–ª–æ—Å—å, –∏ –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ. –Ø –±—ã–ª–æ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏–ª—Å—è –¥–æ–ª–≥–æ –∏ –Ω—É–¥–Ω–æ —Å–æ–±–∏—Ä–∞—Ç—å —ç—Ç–æ –¥–µ–ª–æ –∏–∑ –∏—Å—Ö–æ–¥–Ω–∏–∫–æ–≤, –Ω–æ –¥–µ–ª–∞—Ç—å —ç—Ç–æ –Ω–µ –Ω–∞–¥–æ, —Ç–∞–º –µ—Å—Ç—å –≥–æ—Ç–æ–≤—ã–µ —Å–±–æ—Ä–∫–∏ –ø–æ–¥ Windows. –ü–æ—Å–∫–æ–ª—å–∫—É —É –º–µ–Ω—è –∂–µ–ª–µ–∑–∫–∞ –Ω–∞ Xeon —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π AVX512, –≤–æ—Ç —ç—Ç—É –≤–µ—Ä—Å–∏—é —è –∏ —Å–∫–∞—á–∞–ª.

–ö–æ—Ä–æ—á–µ, —Ç—É–ø–æ –∑–∞–±—Ä–∞—Å—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∞—Ä—Ö–∏–≤–∞ —Å —É—Ç–∏–ª–∏—Ç–∞–º–∏ –≤ –æ–¥–Ω—É –ø–∞–ø–∫—É (–ø—Ä—è–º–æ –≤ –∫–æ—Ä–µ–Ω—å –¥–∏—Å–∫–∞ F:\ –≤ –º–æ—ë–º —Å–ª—É—á–∞–µ)

![image-20250205115905649](assets/image-20250205115905649.png)

–¥–∞ –∑–∞–ø—É—Å–∫–∞–µ–º:

```
F:\>llama-cli.exe -h
```

–í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ (–Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, –ø–æ–¥ —Å–ø–æ–π–ª–µ—Ä–æ–º). –∏—Ö —Ç–∞–º –æ–≤–µ—Ä–¥–æ—Ñ–∏–≥–∞:

```
----- common params -----

-h,    --help, --usage                  print usage and exit
--version                               show version and build info
--verbose-prompt                        print a verbose prompt before generation (default: false)
-t,    --threads N                      number of threads to use during generation (default: -1)
                                        (env: LLAMA_ARG_THREADS)
-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:
                                        same as --threads)
-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range
                                        (default: "")
-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask
--cpu-strict <0|1>                      use strict CPU placement (default: 0)
--prio N                                set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime
                                        (default: 0)
--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)
-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch
                                        (default: same as --cpu-mask)
-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch
--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)
--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime
                                        (default: 0)
--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)
-c,    --ctx-size N                     size of the prompt context (default: 4096, 0 = loaded from model)
                                        (env: LLAMA_ARG_CTX_SIZE)
-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity, -2 = until
                                        context filled)
                                        (env: LLAMA_ARG_N_PREDICT)
-b,    --batch-size N                   logical maximum batch size (default: 2048)
                                        (env: LLAMA_ARG_BATCH)
-ub,   --ubatch-size N                  physical maximum batch size (default: 512)
                                        (env: LLAMA_ARG_UBATCH)
--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =
                                        all)
-fa,   --flash-attn                     enable Flash Attention (default: disabled)
                                        (env: LLAMA_ARG_FLASH_ATTN)
-p,    --prompt PROMPT                  prompt to start generation with
                                        if -cnv is set, this will be used as system prompt
--no-perf                               disable internal libllama performance timings (default: false)
                                        (env: LLAMA_ARG_NO_PERF)
-f,    --file FNAME                     a file containing the prompt (default: none)
-bf,   --binary-file FNAME              binary file containing the prompt (default: none)
-e,    --escape                         process escapes sequences (\n, \r, \t, \', \", \\) (default: true)
--no-escape                             do not process escape sequences
--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by
                                        the model
                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)
--rope-scale N                          RoPE context scaling factor, expands context by a factor of N
                                        (env: LLAMA_ARG_ROPE_SCALE)
--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from
                                        model)
                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)
--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N
                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)
--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training
                                        context size)
                                        (env: LLAMA_ARG_YARN_ORIG_CTX)
--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full
                                        interpolation)
                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)
--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)
                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)
--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)
                                        (env: LLAMA_ARG_YARN_BETA_SLOW)
--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)
                                        (env: LLAMA_ARG_YARN_BETA_FAST)
-dkvc, --dump-kv-cache                  verbose print of the KV cache
-nkvo, --no-kv-offload                  disable KV offload
                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)
-ctk,  --cache-type-k TYPE              KV cache data type for K
                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1
                                        (default: f16)
                                        (env: LLAMA_ARG_CACHE_TYPE_K)
-ctv,  --cache-type-v TYPE              KV cache data type for V
                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1
                                        (default: f16)
                                        (env: LLAMA_ARG_CACHE_TYPE_V)
-dt,   --defrag-thold N                 KV cache defragmentation threshold (default: 0.1, < 0 - disabled)
                                        (env: LLAMA_ARG_DEFRAG_THOLD)
-np,   --parallel N                     number of parallel sequences to decode (default: 1)
                                        (env: LLAMA_ARG_N_PARALLEL)
--rpc SERVERS                           comma separated list of RPC servers
                                        (env: LLAMA_ARG_RPC)
--mlock                                 force system to keep model in RAM rather than swapping or compressing
                                        (env: LLAMA_ARG_MLOCK)
--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not
                                        using mlock)
                                        (env: LLAMA_ARG_NO_MMAP)
--numa TYPE                             attempt optimizations that help on some NUMA systems
                                        - distribute: spread execution evenly over all nodes
                                        - isolate: only spawn threads on CPUs on the node that execution
                                        started on
                                        - numactl: use the CPU map provided by numactl
                                        if run without this previously, it is recommended to drop the system
                                        page cache before using this
                                        see https://github.com/ggerganov/llama.cpp/issues/1437
                                        (env: LLAMA_ARG_NUMA)
-dev,  --device <dev1,dev2,..>          comma-separated list of devices to use for offloading (none = don't
                                        offload)
                                        use --list-devices to see a list of available devices
                                        (env: LLAMA_ARG_DEVICE)
--list-devices                          print list of available devices and exit
-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM
                                        (env: LLAMA_ARG_N_GPU_LAYERS)
-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:
                                        - none: use one GPU only
                                        - layer (default): split layers and KV across GPUs
                                        - row: split rows across GPUs
                                        (env: LLAMA_ARG_SPLIT_MODE)
-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of
                                        proportions, e.g. 3,1
                                        (env: LLAMA_ARG_TENSOR_SPLIT)
-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for
                                        intermediate results and KV (with split-mode = row) (default: 0)
                                        (env: LLAMA_ARG_MAIN_GPU)
--check-tensors                         check model tensor data for invalid values (default: false)
--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified
                                        multiple times.
                                        types: int, float, bool, str. example: --override-kv
                                        tokenizer.ggml.add_bos_token=bool:false
--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)
--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use
                                        multiple adapters)
--control-vector FNAME                  add a control vector
                                        note: this argument can be repeated to add multiple control vectors
--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE
                                        note: this argument can be repeated to add multiple scaled control
                                        vectors
--control-vector-layer-range START END
                                        layer range to apply the control vector(s) to, start and end inclusive
-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`
                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)
                                        (env: LLAMA_ARG_MODEL)
-mu,   --model-url MODEL_URL            model download url (default: unused)
                                        (env: LLAMA_ARG_MODEL_URL)
-hf,   -hfr, --hf-repo <user>/<model>[:quant]
                                        Hugging Face model repository; quant is optional, case-insensitive,
                                        default to Q4_K_M, or falls back to the first file in the repo if
                                        Q4_K_M doesn't exist.
                                        example: unsloth/phi-4-GGUF:q4_k_m
                                        (default: unused)
                                        (env: LLAMA_ARG_HF_REPO)
-hfd,  -hfrd, --hf-repo-draft <user>/<model>[:quant]
                                        Same as --hf-repo, but for the draft model (default: unused)
                                        (env: LLAMA_ARG_HFD_REPO)
-hff,  --hf-file FILE                   Hugging Face model file. If specified, it will override the quant in
                                        --hf-repo (default: unused)
                                        (env: LLAMA_ARG_HF_FILE)
-hfv,  -hfrv, --hf-repo-v <user>/<model>[:quant]
                                        Hugging Face model repository for the vocoder model (default: unused)
                                        (env: LLAMA_ARG_HF_REPO_V)
-hffv, --hf-file-v FILE                 Hugging Face model file for the vocoder model (default: unused)
                                        (env: LLAMA_ARG_HF_FILE_V)
-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment
                                        variable)
                                        (env: HF_TOKEN)
--log-disable                           Log disable
--log-file FNAME                        Log to file
--log-colors                            Enable colored logging
                                        (env: LLAMA_LOG_COLORS)
-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for
                                        debugging)
-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be
                                        ignored.
                                        (env: LLAMA_LOG_VERBOSITY)
--log-prefix                            Enable prefx in log messages
                                        (env: LLAMA_LOG_PREFIX)
--log-timestamps                        Enable timestamps in log messages
                                        (env: LLAMA_LOG_TIMESTAMPS)


----- sampling params -----

--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by
                                        ';'
                                        (default: penalties;dry;top_k;typ_p;top_p;min_p;xtc;temperature)
-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)
--sampling-seq, --sampler-seq SEQUENCE
                                        simplified sequence for samplers that will be used (default: edkypmxt)
--ignore-eos                            ignore end of stream token and continue generating (implies
                                        --logit-bias EOS-inf)
--temp N                                temperature (default: 0.8)
--top-k N                               top-k sampling (default: 40, 0 = disabled)
--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)
--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)
--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)
--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)
--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)
--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1
                                        = ctx_size)
--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)
--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)
--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)
--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)
--dry-base N                            set DRY sampling base value (default: 1.75)
--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)
--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =
                                        context size)
--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers
                                        ('\n', ':', '"', '*') in the process; use "none" to not use any
                                        sequence breakers
--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)
--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)
--mirostat N                            use Mirostat sampling.
                                        Top K, Nucleus and Locally Typical samplers are ignored if used.
                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)
--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)
--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)
-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,
                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',
                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'
--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/
                                        dir) (default: '')
--grammar-file FNAME                    file to read grammar from
-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.
                                        `{}` for any JSON object
                                        For schemas w/ external $refs, use --grammar +
                                        example/json_schema_to_grammar.py instead


----- example-specific params -----

--no-display-prompt                     don't print prompt at generation (default: false)
-co,   --color                          colorise output to distinguish prompt and user input from generations
                                        (default: false)
--no-context-shift                      disables context shift on inifinite text generation (default:
                                        disabled)
                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)
-ptc,  --print-token-count N            print token count every N tokens (default: -1)
--prompt-cache FNAME                    file to cache prompt state for faster startup (default: none)
--prompt-cache-all                      if specified, saves user input and generations to cache as well
--prompt-cache-ro                       if specified, uses the prompt cache but does not update it
-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode
-sp,   --special                        special tokens output enabled (default: false)
-cnv,  --conversation                   run in conversation mode:
                                        - does not print special tokens and suffix/prefix
                                        - interactive mode is also enabled
                                        (default: auto enabled if chat template is available)
-no-cnv, --no-conversation              force disable conversation mode (default: false)
-i,    --interactive                    run in interactive mode (default: false)
-if,   --interactive-first              run in interactive mode and wait for input right away (default: false)
-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\'
--in-prefix-bos                         prefix BOS to user inputs, preceding the `--in-prefix` string
--in-prefix STRING                      string to prefix user inputs with (default: empty)
--in-suffix STRING                      string to suffix after user inputs with (default: empty)
--no-warmup                             skip warming up the model with an empty run
-gan,  --grp-attn-n N                   group-attention factor (default: 1)
                                        (env: LLAMA_ARG_GRP_ATTN_N)
-gaw,  --grp-attn-w N                   group-attention width (default: 512)
                                        (env: LLAMA_ARG_GRP_ATTN_W)
--jinja                                 use jinja template for chat (default: disabled)
                                        (env: LLAMA_ARG_JINJA)
--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's
                                        metadata)
                                        if suffix/prefix are specified, template will be disabled
                                        only commonly used templates are accepted (unless --jinja is set
                                        before this flag):
                                        list of built-in templates:
                                        chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2, deepseek3,
                                        exaone3, falcon3, gemma, gigachat, glmedge, granite, llama2,
                                        llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, megrez, minicpm,
                                        mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7, monarch,
                                        openchat, orion, phi3, phi4, rwkv-world, vicuna, vicuna-orca, zephyr
                                        (env: LLAMA_ARG_CHAT_TEMPLATE)
--chat-template-file JINJA_TEMPLATE_FILE
                                        set custom jinja chat template file (default: template taken from
                                        model's metadata)
                                        if suffix/prefix are specified, template will be disabled
                                        only commonly used templates are accepted (unless --jinja is set
                                        before this flag):
                                        list of built-in templates:
                                        chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2, deepseek3,
                                        exaone3, falcon3, gemma, gigachat, glmedge, granite, llama2,
                                        llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, megrez, minicpm,
                                        mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7, monarch,
                                        openchat, orion, phi3, phi4, rwkv-world, vicuna, vicuna-orca, zephyr
                                        (env: LLAMA_ARG_CHAT_TEMPLATE_FILE)
--simple-io                             use basic IO for better compatibility in subprocesses and limited
                                        consoles

example usage:

  text generation:     llama-cli.exe -m your_model.gguf -p "I believe the meaning of life is" -n 128

  chat (conversation): llama-cli.exe -m your_model.gguf -p "You are a helpful assistant" -cnv


```

### "–†–∞–±–æ—Ç–∞–µ–º"

–ù—É –≤–æ—Ç –ø—Ä—è–º "—Ä–∞–±–æ—Ç–æ–π" —ç—Ç–æ –Ω–∞–∑–≤–∞—Ç—å —Å–ª–æ–∂–Ω–æ, –ø–æ—Ç–æ–º—É —á—Ç–æ —ç—Ç–æ –≤—Å—ë –æ—Ç—á–∞—è–Ω–Ω–æ –º–µ–¥–ª–µ–Ω–Ω–æ.

–°—Ä–∞–∑—É –ø–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª—å —Ü–µ–ª–∏–∫–æ–º –≤ –ø–∞–º—è—Ç—å –Ω–µ –≥—Ä—É–∑–∏—Ç—Å—è, –æ–Ω–∞ –∫–∞–∫ –±—ã "–¥–æ–≥—Ä—É–∂–∞–µ—Ç—Å—è" –ø—Ä–∏ –æ–±—â–µ–Ω–∏–∏.

–ü—Ä–∏ –ø–µ—Ä–≤–æ–º "—Ö–æ–ª–æ–¥–Ω–æ–º" —Å—Ç–∞—Ä—Ç–µ –¥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è –ø—Ä–∏–≥–ª–∞—à–µ–Ω–∏—è –ø—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ—á—Ç–∏ –¥–≤–µ –º–∏–Ω—É—Ç—ã.

–Ø —Å–º–∞—Å—Ç–µ—Ä–∏–ª —Ç–∞–∫—É—é –≤–æ—Ç –Ω–µ—Å–ª–æ–∂–Ω—É—é –æ—Å–Ω–∞—Å—Ç–∫—É, –ø–æ–∫–∞–∑—ã–≤–∞—é—â—É—é –∑–∞–≥—Ä—É–∑–∫—É –ø–∞–º—è—Ç–∏ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞, –Ω—É –∏ —Ç–∞–π–º–µ—Ä –¥–æ –∫—É—á–∏, —á—Ç–æ –± –≤—ã –ø—Ä–æ—á—É–≤—Å—Ç–≤–æ–≤–∞–ª–∏ —ç—Ç—É –±–æ–ª—å (—è –µ—â—ë –≤—ã—Ä–µ–∑–∞–ª –ø—Ä–∏–º–µ—Ä–Ω–æ –º–∏–Ω—É—Ç—É, –ø–æ–∫–∞ —á–∞—Å—Ç—å –º–æ–¥–µ–ª–∏ –≥—Ä—É–∑–∏–ª–∞—Å—å —Å –¥–∏—Å–∫–∞ –≤ –ø–∞–º—è—Ç—å, –æ—Å—Ç–∞–ª—å–Ω–æ–µ –æ—Å—Ç–∞–≤–ª—é –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –∫–∞–∫ –µ—Å—Ç—å):

![](assets/ds4a-OK.gif)

–ö–∞–∫ –≤–∏–¥–∏—Ç–µ, –ø–∞–º—è—Ç—å –∑–∞–ø–æ–ª–Ω–∏–ª–∞—Å—å –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ —Ç—Ä–µ—Ç—å.

–ï—Å–ª–∏ –≤–≤–æ–¥–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã, —Ç–æ –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –¥–æ–≥—Ä—É–∂–∞—Ç—å—Å—è. –ù–µ—Ç —Å–º—ã—Å–ª–∞ –º—É—á–∏—Ç—å –≤–∞—Å –º–µ–¥–ª–µ–Ω–Ω—ã–º –≤—ã–≤–æ–¥–æ–º, —è –±—É–¥—É —Å–Ω–∏–º–∞—Ç—å –∫–∞–¥—Ä –≤ —Å–µ–∫—É–Ω–¥—É, –ø—Ä–æ–∏–≥—Ä—ã–≤–∞—è –≥–∏—Ñ–∫—É —Å –ø—Ä–∏–º–µ—Ä–Ω–æ –¥–≤–∞–¥—Ü–∞—Ç–∏–∫—Ä–∞—Ç–Ω—ã–º —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º. –ü–æ–ø—Ä–æ—Å–∏–º —É –Ω–µ—ë –†—É–Ω–≥–µ-–ö—É—Ç—Ç–∞, —Ö–æ—Ä–æ—à–æ –≤–∏–¥–Ω–æ, –∫–∞–∫ –æ–Ω–∞ –Ω–∞ —Ö–æ–¥—É –∑–∞–ø–æ–ª–Ω—è–µ—Ç —Å–µ–º—å—Å–æ—Ç –≥–∏–≥–∞–±–∞–π—Ç –ø–∞–º—è—Ç–∏ –ø–æ—á—Ç–∏ –ø–æ–¥ –∑–∞–≤—è–∑–∫—É:

![](assets/ds5a.gif)

–ù–∞ –≤—Å—ë –ø—Ä–æ –≤—Å—ë —É –Ω–µ—ë —É—à–ª–æ –¥–µ—Å—è—Ç—å –º–∏–Ω—É—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –±—ã–ª –∑–∞–Ω—è—Ç —á—É—Ç—å –±–æ–ª—å—à–µ —á–µ–º –Ω–∞ –ø–æ–ª–æ–≤–∏–Ω—É, —Ç—É—Ç –≤–æ–∑–º–æ–∂–Ω–æ –ª–∏–±–æ –≥–∏–ø–µ—Ä—Ç—Ä–µ–¥–∏–Ω–≥, –ª–∏–±–æ —Ç–æ, —á—Ç–æ –º—ã —É–ø–∏—Ä–∞–µ–º—Å—è –≤ –ø—Ä–æ–ø—É—Å–∫–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø–∞–º—è—Ç–∏ ‚Äî –∫–∞–∫ –æ—Ç–º–µ—Ç–∏–ª–∏ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ö –∫ –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ–º—É –ø–æ—Å—Ç—É —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –Ω–µ –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ –∏–º–µ—Ç—å –¥–≤–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞, –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—â–∏—Ö –∑–∞ –æ–±—â—É—é –ø–∞–º—è—Ç—å.

–ü–æ–ª–Ω—ã–π –≤—ã–≤–æ–¥:

> Hi, DeepSeek!
> <think>

Hi! How can I assist you today?

> Can you please provide C code of Runge Kutta Method for (x-y)/(x+y)?
> <think>

Certainly! The Runge-Kutta method is a numerical technique used to solve ordinary differential equations (ODEs). For the differential equation dy/dx = (x - y)/(x + y), we can apply the 4th-order Runge-Kutta method (RK4). Here's a C code implementation:

```c
#include <stdio.h>
#include <math.h>

// Define the differential equation dy/dx = (x - y)/(x + y)
double f(double x, double y) {
    return (x - y) / (x + y);
}

// 4th-order Runge-Kutta method
void rungeKutta4(double x0, double y0, double h, double x_target) {
    double x = x0;
    double y = y0;
    double k1, k2, k3, k4;

    printf("Initial conditions: x = %lf, y = %lf\n", x, y);

    while (x < x_target) {
        if (x + h > x_target) {
            h = x_target - x; // Adjust step to reach x_target exactly
        }

        k1 = h * f(x, y);
        k2 = h * f(x + h/2, y + k1/2);
        k3 = h * f(x + h/2, y + k2/2);
        k4 = h * f(x + h, y + k3);

        y = y + (k1 + 2*k2 + 2*k3 + k4)/6;
        x = x + h;

        printf("x = %lf, y = %lf\n", x, y);
    }
}

int main() {
    double x0 = 1.0;   // Initial x
    double y0 = 0.0;   // Initial y
    double h = 0.1;    // Step size
    double x_target = 2.0; // Target x value

    rungeKutta4(x0, y0, h, x_target);
    return 0;
}
```

**Important Notes:**
1. **Initial Conditions:** The current example uses `x0=1.0`, `y0=0.0`, step size `h=0.1`, and integrates up to `x_target=2.0`. Adjust these values as needed.
2. **Division by Zero:** The function `f(x,y)` will have a division by zero if `x + y = 0`. Ensure your initial conditions and step size avoid this scenario.
3. **Adaptive Step Size:** For better accuracy, you might want to implement adaptive step size control, but this example uses a fixed step size for simplicity.
4. **Output:** The code prints the solution at each step. Modify the `printf` statements as needed for your use case.

Let me know if you need further clarification! 

>

–ï—Å—Ç—å –∫–∞–∫ –º–∏–Ω–∏–º—É–º –¥–≤–∞ —Å–ø–æ—Å–æ–±–∞ –∑–∞–ø—É—Å—Ç–∏—Ç—å DeepSeek. –í—ã—à–µ —è –ø–æ–∫–∞–∑–∞–ª —Ä–µ–∂–∏–º "text generation:"

–í—Ç–æ—Ä–æ–π —Ä–µ–∂–∏–º –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è  "chat (conversation)", –≤–∫–ª—é—á–∞–µ—Ç—Å—è —Ñ–ª–∞–≥–æ–º -cnv

```
llama-cli.exe -m DeepSeek-R1.Q8_0-00001-of-00015.gguf -p "You are a helpful assistant" -cnv
```

–í —ç—Ç–æ–º —Ä–µ–∂–∏–º–µ –æ–Ω–∞ —á—É—Ç—å –±–æ–ª–µ–µ "—Ä–∞–∑–≥–æ–≤–æ—Ä—á–∏–≤–∞", –Ω–æ –∏ –∑–∞–Ω–∏–º–∞–µ—Ç —ç—Ç–æ —Ä–∞–∑–∞ –≤ –ø–æ–ª—Ç–æ—Ä–∞ –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ –æ–Ω–∞ —Ñ–æ—Ä–º—É–ª—ã –≤—ã–¥–∞–ª–∞ –∫—Ä–∞—Å–∏–≤–æ –æ—Ñ–æ—Ä–º–ª–µ–Ω–Ω—ã–º–∏.

–ê, —Å—Ç–æ–ø, —è –∂ –º–æ–≥—É –µ—ë —Å–∞–º—É —Å–ø—Ä–æ—Å–∏—Ç—å –ø—Ä–æ –æ—Ç–ª–∏—á–∏–µ —ç—Ç–∏—Ö —Ä–µ–∂–∏–º–æ–≤:

#### –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —Ä–µ–∂–∏–º–∞–º–∏ ¬´–≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞¬ª –∏ ¬´—á–∞—Ç (–¥–∏–∞–ª–æ–≥)¬ª –≤ llama.cpp
–û—Å–Ω–æ–≤–Ω—ã–µ –æ—Ç–ª–∏—á–∏—è —Å–≤—è–∑–∞–Ω—ã —Å —Ñ–æ—Ä–º–∞—Ç–æ–º –≤–≤–æ–¥–∞, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π –º–æ–¥–µ–ª–∏ –∏ —Å—Ü–µ–Ω–∞—Ä–∏—è–º–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –†–∞–∑–±–µ—Ä—ë–º –ø–æ–¥—Ä–æ–±–Ω–æ:

1. –†–µ–∂–∏–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞

  * –¶–µ–ª—å: –°–≤–æ–±–æ–¥–Ω–æ–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ä–∞—Å—Å–∫–∞–∑—ã, –∫–æ–¥, —ç—Å—Å–µ).
  * –§–æ—Ä–º–∞—Ç –≤–≤–æ–¥–∞:

  * –ü—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∏–∞–ª–æ–≥–∞.

  * –ü—Ä–∏–º–µ—Ä: ./main -m model.bin -p "–ë—É–¥—É—â–µ–µ –ò–ò ‚Äî —ç—Ç–æ"
    –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –º–æ–¥–µ–ª—è–º–∏:
    –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–Ω–µ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è —á–∞—Ç–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, llama-7b).
    –í–æ—Å–ø—Ä–∏–Ω–∏–º–∞–µ—Ç –≤–≤–æ–¥ –∫–∞–∫ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç, –±–µ–∑ —É—á—ë—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–∏–∞–ª–æ–≥–∞.
    –í—ã–≤–æ–¥:
    –î–ª–∏–Ω–Ω—ã–π –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ —É—Å–ª–æ–≤–∏—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ (–º–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤, —Å—Ç–æ–ø-—Å–ª–æ–≤–æ).
    –ü—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á: –ö—Ä–µ–∞—Ç–∏–≤–Ω–æ–µ –ø–∏—Å—å–º–æ, —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞.

2. –ß–∞—Ç-—Ä–µ–∂–∏–º
    –¶–µ–ª—å: –ò–º–∏—Ç–∞—Ü–∏—è –¥–∏–∞–ª–æ–≥–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –±–æ—Ç—ã, –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã).
    –§–æ—Ä–º–∞—Ç –≤–≤–æ–¥–∞:
    –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–ø–ª–∏–∫–∏ —Å –º–µ—Ç–∫–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, [INST], <<SYS>>).
    –ü—Ä–∏–º–µ—Ä:
    üìã Copy
    [INST] <<SYS>>–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.<</SYS>> –ß—Ç–æ —Ç–∞–∫–æ–µ –∫–≤–∞–Ω—Ç–æ–≤—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä? [/INST]
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ–ª–∞–≥ --chat –∏–ª–∏ —à–∞–±–ª–æ–Ω—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
    –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –º–æ–¥–µ–ª—è–º–∏:
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è —á–∞—Ç-–º–æ–¥–µ–ª–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, llama-2-7b-chat).
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–∏–∞–ª–æ–≥–∞—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤.
    –í—ã–≤–æ–¥:
    –ö–æ—Ä–æ—Ç–∫–∏–µ —Ä–µ–ø–ª–∏–∫–∏, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —Ç–æ–∫–µ–Ω–∞—Ö –∫–æ–Ω—Ü–∞ –¥–∏–∞–ª–æ–≥–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, </s>).
    –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤).
    –ü—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á: –í–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç, –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤, —Ä–æ–ª–µ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏.
    –ö–ª—é—á–µ–≤—ã–µ —Ä–∞–∑–ª–∏—á–∏—è
    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å	–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞	–ß–∞—Ç-—Ä–µ–∂–∏–º
    –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤–≤–æ–¥–∞	–ü—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç	–†–∞–∑–º–µ—Ç–∫–∞ —Å —Ä–µ–ø–ª–∏–∫–∞–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    –¢–∏–ø –º–æ–¥–µ–ª–∏	–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (–±–µ–∑ —á–∞—Ç-–¥–æ–æ–±—É—á–µ–Ω–∏—è)	–ß–∞—Ç-–º–æ–¥–µ–ª–∏ (–Ω–∞–∑–≤–∞–Ω–∏—è —Å -chat)
    –£—Å–ª–æ–≤–∏—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏	–ú–∞–∫—Å. —Ç–æ–∫–µ–Ω—ã, —Å—Ç–æ–ø-—Å–ª–æ–≤–∞	–¢–æ–∫–µ–Ω—ã –∫–æ–Ω—Ü–∞ —Ä–µ–ø–ª–∏–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, </s>)
    –ö–æ–Ω—Ç–µ–∫—Å—Ç	–£–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –≤—Ä—É—á–Ω—É—é	–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∏—Å—Ç–æ—Ä–∏—è
    –§–ª–∞–≥ –≤ –∫–æ–º–∞–Ω–¥–µ	–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é	--chat
    –ü—Ä–∏–º–µ—Ä—ã –∫–æ–º–∞–Ω–¥
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞:
    üìã Copy
    ./main -m llama-7b.bin -p "–û–±—ä—è—Å–Ω–∏ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ:" -n 512
    –ß–∞—Ç-—Ä–µ–∂–∏–º:
    üìã Copy
    ./main -m llama-2-7b-chat.bin --chat -p "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: –ü—Ä–∏–≤–µ—Ç\n–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"
    –ü–æ–ª–µ–∑–Ω—ã–µ —Å–æ–≤–µ—Ç—ã
    –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —á–∞—Ç-—Ä–µ–∂–∏–º —Ç–æ–ª—å–∫–æ —Å –º–æ–¥–µ–ª—è–º–∏, –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, llama-2-chat).
    –î–ª—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑–±–µ–≥–∞–π—Ç–µ —Ñ–ª–∞–≥–∞ --chat, —á—Ç–æ–±—ã –Ω–µ –Ω–∞—Ä—É—à–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ.
    –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, system –∏–ª–∏ user) –ø–æ–º–æ–≥–∞—é—Ç —É–ª—É—á—à–∏—Ç—å –æ—Ç–≤–µ—Ç—ã —á–∞—Ç-–º–æ–¥–µ–ª–µ–π.
    –ï—Å–ª–∏ –Ω—É–∂–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ—è—Å–Ω–µ–Ω–∏—è, —Å–ø—Ä–∞—à–∏–≤–∞–π—Ç–µ! üòä

  

> Hi, DeepSeek!
> <think>

Hi! How can I assist you today? ÔøΩ

> Can you please provide C code of Runge Kutta Method for (x-y)/(x+y)?
> <think>

Certainly! To implement the Runge-Kutta method (specifically the 4th order, RK4) for solving the differential equation \( \frac{dy}{dx} = \frac{x - y}{x + y} \), here's a C code example. This code will approximate the solution at a specified point given an initial condition.

```c
#include <stdio.h>
#include <math.h>

// Define the differential equation dy/dx = (x - y)/(x + y)
double f(double x, double y) {
    return (x - y) / (x + y);
}

// Runge-Kutta 4th Order Method
void rungeKutta4(double x0, double y0, double h, double x_target) {
    double x = x0;
    double y = y0;
    double k1, k2, k3, k4;

    printf("Initial point: x = %.4f, y = %.4f\n", x, y);

    while (x < x_target) {
        // Ensure step size doesn't exceed the target
        if (x + h > x_target) {
            h = x_target - x;
        }

        k1 = h * f(x, y);
        k2 = h * f(x + h/2, y + k1/2);
        k3 = h * f(x + h/2, y + k2/2);
        k4 = h * f(x + h, y + k3);

        y = y + (k1 + 2*k2 + 2*k3 + k4)/6;
        x = x + h;

        printf("x = %.4f, y = %.4f\n", x, y);
    }
}

int main() {
    double x0 = 1.0;  // Initial x
    double y0 = 0.0;  // Initial y
    double h = 0.2;    // Step size
    double x_target = 2.0; // Target x to approximate y

    rungeKutta4(x0, y0, h, x_target);

    return 0;
}
```

### Explanation:
1. **Function `f`**: Implements the differential equation \( \frac{dy}{dx} = \frac{x - y}{x + y} \).
2. **RK4 Method**:
   - Takes initial conditions `(x0, y0)`, step size `h`, and target `x_target`.
   - Iteratively computes the next y using the RK4 formulas:
     [
     k_1 = h \cdot f(x_n, y_n),
     ]
     [
     $$
     k_2 = h \cdot f(x_n + \frac{h}{2}, y_n + \frac{k_1}{2}),
     $$
     ]
     [
     k_3 = h \cdot f(x_n + \frac{h}{2}, y_n + \frac{k_2}{2}),
     ]
     [
     k_4 = h \cdot f(x_n + h, y_n + k_3),
     ]
     [
     y_{n+1} = y_n + \frac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4).
     ]
   - Adjusts the step size near the target to avoid overshooting.

### Usage:
- Modify `x0`, `y0`, `h`, and `x_target` in `main()` as needed.
- Compile with `gcc -o rk4 rk4.c -lm` (ensure math library linkage).

Let me know if you need further adjustments! ÔøΩ

>

–†—É—Å—Å–∫–∏–π –ø–æ–Ω–∏–º–∞–µ—Ç:

![image-20250205150603692](assets/image-20250205150603692.png)

–ì—Ä–∞–º–º–∞—Ç–∏–∫—É –ø—Ä–∞–≤–∏—Ç, –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç:

![image-20250205151604144](assets/image-20250205151604144.png)

–í —Å—Ç–∏—Ö–∞—Ö —Ç–æ–∂–µ –º–æ–∂–µ—Ç. –ü–æ—Ö–æ–∂–µ, –º–æ—è –¥–µ–ª–æ–≤–∞—è –∫–æ—Ä—Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ü–∏—è —Å—Ç–∞–Ω–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–µ–µ.

–ö–æ—Ä–æ—á–µ —É –º–µ–Ω—è –ø–æ–¥ —Å—Ç–æ–ª–æ–º –∑–∞–≤—ë–ª—Å—è –ª–∏—á–Ω—ã–π —Å–µ–∫—Ä–µ—Ç–∞—Ä—å, —Ö–æ—Ç—å –∏ —á—É—Ç–∫–∞ —Ç–æ—Ä–º–æ–∑–Ω—É—Ç—ã–π. –ü—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Ä–∞–±–æ—Ç—ã –≤—ã–¥–∞—ë—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É, –æ–∫–æ–ª–æ –æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ —Å–µ–∫—É–Ω–¥—É:

llama_perf_sampler_print:    sampling time =      52.33 ms /   209 runs   ( 0.25 ms per token,  3993.66 tokens per second)
llama_perf_context_print:        load time =   89790.30 ms
llama_perf_context_print: prompt eval time =   87115.12 ms /   112 tokens (  777.81 ms per token,     1.29 tokens per second)
llama_perf_context_print:        eval time = 1883493.69 ms /  1777 runs   ( 1059.93 ms per token,     0.94 tokens per second)
llama_perf_context_print:       total time = 3136520.40 ms /  1889 tokens

–ö–æ–º—É –Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –∫–æ–Ω—Å–æ–ª—å, –º–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –∏ –≤–æ—Ç —Ç–∞–∫:

```
llama-server.exe -m DeepSeek-R1.Q8_0-00001-of-00015.gguf --port 8082
```

–≠—Ç–∞ —à—Ç—É–∫–∞ –ø–æ–¥–Ω–∏–º–µ—Ç –≤–µ–± —Å–µ—Ä–≤–µ—Ä (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –Ω–∞ –ø–æ—Ä—Ç—É 8080, –Ω–æ –æ–Ω —É –º–µ–Ω—è –∑–∞–Ω—è—Ç, —Ç–∞–∫ —á—Ç–æ 8082):

![image-20250205152938717](assets/image-20250205152938717.png)

–ù–æ –≤–ø–µ—á–∞—Ç–ª—è–µ—Ç, –¥–∞–∂–µ –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –Ω–∏–∑–∫—É—é —Å–∫–æ—Ä–æ—Å—Ç—å.