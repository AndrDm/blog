Привер, Хабр!

В копилку постов про DeepSeek, о котором не писал разве что совсем ленивый хочу забросить ещё пять копеек в виде практического отчёта о инсталляции на Xeon, о котором меня попросили в комментариях. Кому любопытно - может заглянуть под кат, ну а тем, кто уже выполнил упражнение по установке - будет совершенно неинтересно.

Прикоснуться к ИИ.

Спойлер для экономии времени читающих - я просто скачаю DeepSeek и запущу его через llama.cpp, на какую-либо научную новизну этот пост совершенно не претендует. Зачем это на хабре? Просто в комментариях к посту "" я спросил "имеете ли смысл попробовать на том железе, что у меня есть", и некоторое количество "плюсиков" говорит о том, что кому-нибудь это будет интересно, ну а раз паззл сложился - о том и пост, просто развёрнутое продолжение к предыдущему.

Второй спойлер - да это работает. Но очень медленно. Но работает.

### Железо

Упражняться я буду вот на такой конфигурации:

HP z8 рабочая станция примерно четырёхлетней давности. Два процессора Xeon Gold, 768 GB памяти DDR4, терабайтный NVMe SSD Samsung. Видеокарт там две - NVidia Quadro K4200 да AMD, но использоваться как GPU они не будут (смысла в общем нет).

Комп остался как тестовый от одного проекта,  в котором надо быстро обрабатывать полтерабайта картинок, прилетающих от нескольких скоростных камер, теперь просто пылится под столом и эпизодически используется для упражнений.

Бенчмарки:

SSD.

### Скачиваем DeepSeek R0

Я бы мог просто написать "скачайте DeepSeek", но тут есть небольшой нюанс. Дело в том, что на работе у меня все искусственные интеллекты старательно заблокированы злым админом, охраняющим интеллектуальную собственность. У меня нет онлайн доступа ни к ChatGPT, ни к Perplexity, ни к DeepSeek, равно как все дропбоксы, гуглодрайвы, онлайн заметочники - блокировано решительно всё. Причём не только на уровне прокси, но также фактически осуществляется MITM подменой сертификата, поэтому скачать с huggingface я ничего не могу, получая отлуп 500. Издержки работы в большой компании. Так что качать я буду дома. А дома у меня всего-навсего 100 Мбит (хоть и оптоволокно), так что процесс не быстрый (планируйте больше суток).

Поскольку репозиторий содержит несколько моделей, то я решил, что я самый умный, и слегка погуглив нашёл способ выдернуть через гит отдельную папку, а не всю репу.

Вот так это делается:

```
git ...
```

Я включил старый комп с файлопомойкой , проверил там свободное место, выполнил команды выше и пошёл спать. Наутро я обнаружил, что туда прилетели обновления и он перезагрузился. Ну, бывает.

Лирическое отступление - если вы хотите временно отказаться от обновлений, то просто включите Metered Network, это самый наипростейший способ.

Я это сделал, и выполнив команды второй раз, обнаружил, что "докачки" там нет и в помине, git начал качать с нуля (это видно в кеше). Тут я вспомнил, что у меня валяется древний QNAP NAS, там есть качалка, но тут меня ждала другая засада - раз в сутки провайдер сбрасывает соединение, при этом NAS вываливал ошибку и начинал скачку снова. С торрентами он худо-бедно справлялся, а вот с https нет. Я уже хотел было отказаться от затеи, но если я во всеуслышаение напишу здесь, что не смог скачать шестьсот гиг, то надо мной будет ржать весь хабр, и я буду подвергнут публичному остракизму, так что пришлось расчехлить  менеджер закачек, которым я не пользовался уже много лет.

Я это к тому, что если захотите скачать — заранее проверьте, что вы можете докачивать при обрыве соединения.

Как бы то ни было, вот репозиторий, вот модель, а под спойлером - прямые ссылки на список файлов, которые можно скормить любому менеджеру, я пользовался [JDownloader 2](https://jdownloader.org/download/index).

Пока идёт скачивание, чтоб было не скучно - могу предложить загадку.

Во второй половине девяностых, мы, новоиспечённые выпускники - физики иногда собиралсь вместе, распивали крепкие спиртные напитки и иногда смотрели фильмы на видеокассетах. В числе наиболее популярных был фильм, который мы называли не иначе как "это наше кино, про эти, про ГИГАБАЙТЫ!" Угадайте, о каком фильме, выпущенном в середине девяностых, шла речь?

Как бы то ни было, через пару дней и ночей на терабайтном диске у меня лежали пятнадцать заветных файлов. Испытал странное ощущение - это у меня в руках почти весь запас знаний?

### Запускаем.

Вообще говоря существует несколько способов "запустить" DeepSeek - LM Studio, Ollama, llama.ccp и OpenVINO. Я попробовал навскидку все, но без фанатизма.

Самый наипростейший - LM Studio. Актуальная версия 3.9.6. Я попробовал поставить дома, всё без проблем, как демка скачивается простенькая DeepSeek R1 Distilled (Qwen 7B) на 4 с половиной гига, что позволяет запустить это дело даже на моём древнем ноуте (32 ГБ, процессор i7-4940MX). Я попробовал - даже работает, но очень медленно. Файлы моделей по умолчанию лежат в C:\Users\Andrey\.\\lmstudio\models\lmstudio-community\DeepSeek-R1-Distill-Qwen-7B-GGUF.

Я закинул на диск инсталлятор, но вот на hp z8 меня ждал облом - "тестовая" Qwen 7B хоть и загружалась, но вываливалась в "неизвестную ошибку" после первого же промпта, а Q0 отказалась загружаться вовсе, сославшись на нехватку ресурсов. Дальше я пробовать пока не стал и отложил красивую игрушку в сторону.

Ollama в основном рассчитана на то, что скачивать модель вы будете через неё же (что невозможно в моём случае, а перекачивать всё ещё раз - извините), 

```
Welcome to Ollama!

Run your first model:

        ollama run llama3.2

PS C:\Windows\System32> ollama run deepseek-r1:1.5b
pulling manifest
pulling aabd4debf0c8... 100% ▕████████████████████████████████████████▏ 1.1 GB
pulling 369ca498f347... 100% ▕████████████████████████████████████████▏  387 B
pulling 6e4c38e1172f... 100% ▕████████████████████████████████████████▏ 1.1 KB
pulling f4d24e9138dd... 100% ▕████████████████████████████████████████▏  148 B
pulling a85fe2a2e58e... 100% ▕████████████████████████████████████████▏  487 B
verifying sha256 digest
writing manifest
success
>>> hi!
<think>

</think>

Hello! How can I assist you today? 😊
```

а вот как "подоткнуть" ей уже скачанные файлы GGUF - тут чуть сложнее. То есть место-то где они должны лежать я знаю C:\Users\Andrey\\.ollama\models, но там модель надо кидать в папку \blobs, при этом переименовать файл в sha256-<хеш>, кроме того,  надо добавить конфигурационный файл в \manifests\registry\\.ollama.ai\library\deepseek-r1, в общем тут надо "приготовить" модель через 

\> ollama create <your-model-name-here> -f Modelfile, но перед этим вам придётся смержить все пятнадцать файлов вместе через gguf-split --merge infile-00001-of-0000N.gguf outfile.gguf, в общем ну его в топку такие упражнения.

А вот с llama.ccp всё получилось, и очень просто. Я было приготовился долго и нудно собирать это дело из исходников, но делать это не надо, там есть билды под Windows. Поскольку у меня железка на Xeon, то есть AVX512, вот эту версию я и скачал.

Короче, тупо забрасываем файлы модели и содержимое архива с утилитами в одну папку да запускаем:

```
cli
```

Все параметры командной строки (на английском, под спойлером):

```
<spoiler>
```

### "Работаем"

Ну вот прям "работой" это назвать сложно, потому что это всё отчаянно медленно.

Сразу после запуска модель целиком в память не грузится, она как бы "догружается" при общении.

Вот занятая память